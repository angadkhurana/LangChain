{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import chromadb\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import MathpixPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1 of loading pdf\n",
    "loader = PyPDFLoader(r\"C:\\Users\\angad\\OneDrive\\Desktop\\AML\\thesis\\MSc_Thesis_02298815.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\angad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# method 2 of loading pdf\n",
    "loader = UnstructuredPDFLoader(r\"C:\\Users\\angad\\OneDrive\\Desktop\\AML\\thesis\\MSc_Thesis_02298815.pdf\")\n",
    "# loader = MathpixPDFLoader(r\"C:\\Users\\angad\\OneDrive\\Desktop\\AML\\thesis\\MSc_Thesis_02298815.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredMarkdownLoader(r\"C:\\Users\\angad\\OneDrive\\Desktop\\AML\\thesis\\MSc_Thesis_02298815.md\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Federated Learning for Short-Term\\n\\nLoad Forecasting\\n\\nAuthor\\n\\nA. Khurana\\n\\nCID: 02298815 Supervised by\\n\\nDr Fei Teng\\n\\nA Thesis submitted in fulfillment of requirements for the degree of\\n\\nMaster of Science in Applied Machine Learning\\n\\nDepartment of Electrical and Electronic Engineering\\n\\nImperial College London\\n\\n2023\\n\\nAbstract\\n\\nElectricity is the backbone of today’s society. It powers everything from houses to industries and businesses. The core infrastructure behind electricity is the grid which requires a continual balance between supply and demand for optimal functioning. This necessitates the accurate short-term forecasting of the load (STLF) consumed. In recent years, deep learning has been employed with federated learning for STLF as federated learning allows collaboration of clients without the need to aggregate their data which is difficult due to privacy-related and logistical concerns.\\n\\nThis study analyses the impact of the network topology of federated learning on the performance of the predictive model for the case of STLF. Furthermore, since not all clients benefit from fed- erated learning, the study of predictive models for distinguishing benefitting from non-benefitting clients is also carried out.\\n\\nThe ring topology results in the best performance in terms of the number of benefitting clients and reduction in loss metrics, however with a slower convergence speed. In terms of prediction of the non-benefitting clients, random forest with class weights was discovered to be the most effective achieving a balanced accuracy of 86%, precision of 82%, and a recall of 82% as well.\\n\\nThis research establishes the importance of STLF, identifies the best network topology for optimal performance, and proves the feasibility of predicting the non-benefitting clients.\\n\\nDeclaration of Originality\\n\\nI hereby declare that the work presented in this thesis is my own unless otherwise stated. To the best of my knowledge the work is original and ideas developed in collaboration with others have been appropriately referenced.\\n\\nCopyright Declaration\\n\\nThe copyright of this thesis rests with the author and is made available under a Creative Commons Attribution Non-Commercial No Derivatives licence. Researchers are free to copy, distribute or transmit the thesis on the condition that they attribute it, that they do not use it for commercial purposes and that they do not alter, transform or build upon it. For any reuse or redistribution, researchers must make clear to others the licence terms of this work.\\n\\nAcknowledgments\\n\\nI would firstly like to thank my supervisor Dr Fei Teng for encouraging thought provoking discus- sions and providing incomparbale guidance throughout this thesis. I am also very thankful for Mr Xu’s for valuable inputs and insights on this thesis.\\n\\nFinally, I am very grateful to my friends and family for continuously supporting me throughout this degree.\\n\\nContents\\n\\n**Abstract i Declaration of Originality iii Copyright Declaration v Acknowledgments vii List of Acronyms xi List of Figures xiii List of Tables xv\\n\\n**1 Introduction 1\\n\\n**2 Literature Review 5\\n\\n2.1 Load Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\nConventional Forecasting Methods . . . . . . . . . . . . . . . . . . . . . . . 6\\n\\nHistorical Methods: Adapted Conventional Forecasting Methods . . . . . . 7\\n\\nDeep Learning for Load Forecasting . . . . . . . . . . . . . . . . . . . . . . 10\\n\\nFederated Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n\\nFederated Learning for Short-Term Load Forecasting . . . . . . . . . . . . . 16\\n\\n**3 Methodology 19\\n\\nDataset: Description and Processing . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n\\nDeep Learning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\nFederated Learning Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n\\nNetwork Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\nEvaluation Methods and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n\\nEvaluation of Network Topologies . . . . . . . . . . . . . . . . . . . . . . . 27\\n\\nEvaluating the Feasibility of Predicting Non-Benefitting Clients . . . . . . . 27\\n\\n**4 Results and Discussion 35\\n\\nEvaluation of network topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n\\nLoss Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n\\nConvergence time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n\\nPredicting Non-Benefitting Clients . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n\\n**Conclusions 47\\n\\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n\\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n\\n**Bibliography 49\\n\\nList of Acronyms\\n\\nFL Federated Learning\\n\\nMAPE Mean Absolute Percentage Error MAE Mean Absolute Error\\n\\nRMSE Root Mean Square Error\\n\\nMSE Mean Square Error\\n\\nMMSE Minimum Mean Square Error\\n\\nLMS Least Mean Square\\n\\nFedAVG Federated Averaging\\n\\nFedSGD Federated Stochastic Gradient Descent FedPROX Federated Proximal\\n\\nTP True Positive\\n\\nTN True Negative\\n\\nFP False Positive\\n\\nFN False Negative\\n\\nTPR True Positive Rate\\n\\nFPR False Positive Rate\\n\\nROC Receiver Operating Characteristic AUC Area Under the Curve\\n\\nFNN Forward Neural Networks\\n\\nLSTM Long Short-Term Memory\\n\\nRNN Recurrent Neural Network\\n\\nAR Autoregressive\\n\\nMA Moving Average\\n\\nARMA Autoregressive Moving Average\\n\\nARMAX Autoregressive Moving Average with Exogenous Variables\\n\\nARIMA Autoregressive Integrated Moving Average\\n\\nARIMAX Autoregressive Integrated Moving Average with Exogenous Variables ARIMAX Autoregressive Integrated Moving Average with Exogenous Variables ADAM Adaptive Movement Estimation\\n\\nRF Random Forest\\n\\nSVM Support Vector Machine\\n\\nMLP Multilayered Preceptron Neural Network\\n\\nSTLF Short Term Load Forecasting\\n\\nLTLF Long Term Load Forecasting\\n\\nIID Independent and Identically Distributed\\n\\nList of Figures\\n\\nRNN Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n\\nLSTM Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n\\nExample load profile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\nFederated learning setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n\\nNetwork topology examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n\\nSVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n\\nRandom Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n\\nROC curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n\\nPrecison-Recall curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n\\nNumber of clients benefitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n\\nMAPE improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n\\nMean MAPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n\\nMAE improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n\\nMean MAE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n\\nRMSE improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n\\nMean RMSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n\\nEigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n\\nLearning curve (seed 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n\\nLearning curve (seed 3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n\\nLearning curve (seed 3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n\\nFiedler Eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n\\nPCA 3D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n\\nPCA 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n\\nPrecision-recall curve for random forest . . . . . . . . . . . . . . . . . . . . . . . . 44\\n\\nROC curve for random forest 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n\\nPrecision-recall curve for random forest . . . . . . . . . . . . . . . . . . . . . . . . 45\\n\\nROC curve for random forest 2D . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n\\nList of Tables\\n\\n3.1 Summary of the hyperparameters and settings utilised for the federated deep learn-\\n\\ning setup. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\nAverage MAPE, MAE, and RMSE improvements for six seeds for the four network\\n\\ntopologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n\\nThe list of IDs for the non-benefitting clients for a set of 32 clients corresponding to\\n\\nseed 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n\\nPerformance metrics for all four classification models. . . . . . . . . . . . . . . . . 45\\n\\nFeature importances for the random forest with class weights . . . . . . . . . . . . 45\\n\\nIntroduction\\n\\nPower and electricity are the foundations of modern society, powering homes, businesses, and industries. Reliable electricity supply heavily depends on the finely tuned process of power gen- eration, distribution, and management. A crucial aspect of these processes is load forecasting, a critical task involving electricity demand prediction to ensure efficient and stable power grids. It is therefore first necessary to fully understand the importance of load forecasting. Then explor- ing conventional methods for load forecasting, and finally delved into deep learning methods. In particular, federated learning and investigating the potential applications of federated learning in load forecasting.\\n\\nTo put it simply, load forecasting is the prediction of future electrical power demand using historical data for a given geographical region. Load forecasting is necessary to allow for efficient power grid management by enabling utilities to optimise power generation and distribution, main- tenance schedule planning, allocating resources effectively, and most importantly ensuring an equal supply and demand of electricity. An accurate load forecast also helps to mitigate blackouts; min- imise wasted energy and increase the cost-effectiveness of a power system. With recent impetus on the integration of renewable energy sources as well as a general increase in the usage of electricity, an accurate load forecast has never been more important. Predicting the load consumption is difficult as it is, but the addition of private grid connections to the main grid introduces more volatility and unpredictability, adequately emphasising the importance of load forecasting.\\n\\nTraditionally, load forecasting has relied on statistical and econometric models. The simplest of such models is linear regression which uses only the historical load data for future predictions [1]. However, load forecasting is a complex procedure that is affected by various external factors, such\\n\\n1  as ambient weather and time of day. To incorporate such variables, multivariate linear regression known as multiple regression was used [2]. However, load data has inherent temporal which are\\n\\nnot exploited using regression techniques. ARIMA (AutoRegressive Integrated Moving Average) methods have shown increased performance accuracies for load forecasting compared to regression [3]. The multivariant version of ARIMA known as ARIMAX incorporates the external factors into the forecasting [4]. A thorough investigation into the statistical methods is conducted in Chapter 2. These techniques assume linearity, but deep learning offers better forecasting accuracy through non-linear modeling.\\n\\nDeep learning is a branch of machine learning that uses neural networks with multiple layers to automatically learn and extract complex patterns and representations from data, enabling the development of advanced AI systems capable of tasks like image recognition and natural language understanding. Deep learning is capable of capturing both temporal and spatial patterns in the load data enabling more dynamic load forecasting.\\n\\nHowever, load forecasting relies on historical load profiles, this data may contain personal identifiable information about clients. Therefore, load forecasting is inherently a privacy issue. Even if data is anonymised it may still be possible to identify individuals from external data sources (data leakage). In the interest of client privacy and security, a branch of deep learning known as federated learning has recently been used for load forecasting.\\n\\nFederated learning is a decentralised machine learning approach where a global model is trained collaboratively across multiple decentralised devices or servers, allowing data to remain local while still benefiting from collective model improvements. The main method for this remote collaboration is the averaging of the weights received from multiple clients during the gradient descent procedure. This is done using a specific aggregation algorithm such as Federated Averaging (FedAVG) [5]. A prominent advantage of federated learning is that it aligns with the decentralised nature of electricity grids, as data sources are distributed across different regions and stakeholders. Accurate and robust forecasting models can be made using federated learning to access the distributed data. Another advantage is that it promotes collaboration among utilities and grid operators enabling for more accurate forecasting accuracy without the risk of sharing proprietary information.\\n\\nA very important characteristic of federated learning is the way the clients are connected to each other for weight sharing. This is parametrised using a network topology. Federated learning is significantly affected by this network topology. Furthermore, it has been observed that fairness is an open challenge as everybody does not benefit by taking part in federated learning process\\n\\n[6]. Here, benefit means an improvement in loss metrics after taking part in a federated learning 1 setup.\\n\\nTo date, there has been no research about the impact of network topology on the performance of federated learning for load forecasting. Additionally, no research has been done to analyse which clients do not benefit and if it is feasible to predict them beforehand. Thus, this study aims to tackle these two main questions. The main focus of the study will be on short-term load forecasting (STLF) which refers to load forecasting from a day up to a week ahead.\\n\\nLiterature Review\\n\\nContents\\n\\n**2.1 Load Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\nConventional Forecasting Methods . . . . . . . . . . . . . . . . . . . . . 6\\n\\nHistorical Methods: Adapted Conventional Forecasting Methods . . . . 7\\n\\nDeep Learning for Load Forecasting . . . . . . . . . . . . . . . . . . . . . 10\\n\\nFederated Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n\\nFederated Learning for Short-Term Load Forecasting . . . . . . . . . . . 16\\n\\nThe aim of this chapter is to thoroughly introduce the topics of Load Forecasting in power system and federated learning by reviewing the existing literature. Firstly, the topic of Load Forecasting is analysed by discussing its necessity, the techniques that have been used for it and the motivation behind the incorporation of deep learning methods for load forecasting in the recent years. Subsequently, the nuances of federated learning along with the motivation behind utilising it for load forecasting is discussed.\\n\\n2.1 Load Forecasting\\n\\nThe optimal functioning of the electricity infrastructure is dependent on the continual balance between its demand and supply. This implies that accurate load forecasting is imperative for taking crucial decisions considering the sale and purchase of electricity, emergency preparedness, operations of generators and the distribution system and the integration of renewable energy. Accurate load forecasting is also highly beneficial at a single household level as well as it can inform consumers of any potential saving opportunities. Load forecasting can be done at various\\n\\nLOAD FORECASTING 17\\n\\ntime scales from seconds to years according to the task in hand. Forecasting up to one day or\\n\\nweek ahead is termed short-term load forecasting (STLF) [7]. This project is concerned with 2\\n\\nhourly prediction of the load at an individual house level (STLF). Over the years, a wide variety of predictive methods have been employed for load forecasting. These approaches can be categorised into three primary groups: conventional forecasting methods, adapted conventional methods, and deep learning methods.\\n\\nConventional Forecasting Methods\\n\\nThe electricity consumed is affected by various external factors such as the ambient temperature, human activities, seasonal factors, etc. which makes the task of accurate forecasting extremely challenging. One of the earliest and most basic methods for load forecasting is known as linear regression.\\n\\nThe basic principle for load forecasting using linear regression is to model how load consump- tion is influenced by various factors, including weather conditions, types of days, and customer categories (such as residential homes or industrial facilities), which can be modelled using\\n\\nL(t) = Ln(t) + aixi(t) + (t): (2.1) Ln is the normalised load time at time t, ai are the weights of the linear regression and xi are the\\n\\nindependently varying factors such as ambient temperature, holidays, days of the week etc. Added\\n\\nwhite noise is modelled as (t). The regression model either has n = 24 or n = 168, where n is the number of previous observations used to make the prediction (using either the past day or the\\n\\npast week’s data) [8].\\n\\nThe model’s precision hinges on its capability to accurately forecast plausible future situa- tions, like weather patterns, based on historical data. For precise load predictions, it’s crucial to encompass both day-to-day load fluctuations and yearly load expansion. To factor in this data, transformation methods are employed. Essentially, the estimation of the transformation function entails using data points from the previous year. This enables the function to transform the data points into a fresh set while retaining the initial arrangement of temperature-load relationships observed in the prior year. Detailed examinations utilising transformed linear regression for load forecasting are widely available in existing literature. Gross and Galina, for instance, demonstrate the technique’s use for Short-Term Load Forecasting (STLF) [9]. Similarly, a comparable approach\\n\\nis applied in long-term load forecasting by Ding [10]. The main limitation of linear regression in\\n\\nload forecasting is its inability to adequately model nonlinear relationships intrinsic to load pat-\\n\\n2 terns involving factors such as weather, time of day, and economic circumstances. Moreover, load\\n\\nforecasting necessitates the simultaneous modeling of multiple variables, like weather and time of\\n\\nday, to enhance prediction accuracy. To accommodate these multiple variables, multiple regression\\n\\nis employed instead.\\n\\nIn [1] Papalexopoulos and Hesterberg discuss a multiple regression approach that used the data\\n\\nfrom the last 15 days. The model generates an initial forecast for the daily peak load, which\\n\\nis subsequently used for the creation of initial hourly forecasts. It employs the initial daily peak\\n\\nforecast, selects the maximum from the initial hourly forecasts, and considers the most recent error\\n\\nfrom the initial peak forecast. The main limitation of Papalexopoulos and Hesterberg’s approach is\\n\\nthat it requires a large amount of previous data for STLF. The mathematical formulation and de-\\n\\ntailed load forecasting experiments for multiple regression can be readily found in the literature[11].\\n\\nNevertheless, the basic formulation is given as\\n\\nY(t) = Vtat + t: (2.2)\\n\\nY(t) corresponds to the cumulative recorded load, Vt encompasses the essential variables required for prediction, including parameters like temperature, wind speed, and day type, and at symbolises the vector of transposed regression coefficients. As before, t denotes the added noise vector. This arrangement was employed by Mbamalu and El-Hawary in their application of the least squares technique for multiple regression [2]. Although multiple regression can encompass several variables\\n\\nfor load prediction, it falls short in capturing the inherent autocorrelation and seasonality within the data, which is particularly present in Short-Term Load Forecasting (STLF). Additionally, load data display temporal interdependencies, implying that the present load values are influenced by previous ones. This temporal aspect remains unused in multiple regression but can be harnessed using time series techniques like AutoRegressive Moving Average (ARMA). ARMA methods fall under the adapted conventional methods and are discussed in the subsequent section.\\n\\nHistorical Methods: Adapted Conventional Forecasting Methods\\n\\nTo model the load profile using the autoregressive (AR) approach, the current load (L^k) is simply assumed to be the weighted average of the previous loads (Lk i). This can be modelled as\\n\\nXm\\n\\nL^k = akiLk i + k: (2.3)\\n\\ni=1\\n\\nIn (2.3) an m order AR model is formulated with the random load disturbance modelled as k,\\n\\nwith the unknown weights given as aki;i = 1;:::;m. The unknown coefficients can easily be found 2\\n\\nusing any adaptive filtering technique. One such example is using the stochastic gradient descent (SGD) approach in the form of the LMS algorithm as used in [2]. An example implementation of\\n\\nthe AR model was conducted by Liu in [12]. An AR model predicts a variable’s future values based solely on its own past values. It’s a simple model that assumes the future values of the variable\\n\\nare linearly dependent on its past values. This can be suitable when the load data has a clear autocorrelation structure and is influenced primarily by its recent historical values. However, in practical situations, a simplistic linear relationship falls short when it comes to modeling complex scenarios like extreme weather variations. To address this limitation, additional complexity is introduced through the incorporation of the Moving Average (MA) component. This addition culminates in the formation of the final AutoRegressive Moving Average (ARMA) model.\\n\\nThe MA component captures the relation between the current load and the previous forecasting errors (residuals). It helps account for the short-term deviations from the predicted values that might be due to factors not accounted for by the AR component. Therefore, an ARMA model represents the current value of a time series load (Lt) in terms of the values of the load at previous times (Lt i) and the previous values of the additive white noise t i\\n\\nXp Xq\\n\\nLt = c + iLt i + j t j + t: (2.4)\\n\\ni=1 j=1\\n\\nAn ARMA model of order (p;q) is defined in (2.4). Where, c simply represents a constant term and i and j represent the unknown coefficients of ARMA model. The methods for solving for the ARMA coefficients can be readily found in the literature. However, the most popular method\\n\\nis the Box-Jenkins method as used by Barakat et al. in [13]. In essence, the Box-Jenkins method can be split into five main steps. First, preliminary analysis is conducted to determine the order of the ARMA model (values of p and q in (2.4)) through analysing the autocorrelation function (ACF) and the partial autocorrelation function (PACF). Subsequently, an arbitrary ARMA model of the determined order is fitted to the data. Next, the coefficients of the ARMA model ( i and j ) are estimated using a least squares approach. Following that, the fitted model is checked for\\n\\nfit and for any signs of non-stationarity or heteroskedasticity. If necessary, the model is modified\\n\\nor the data is transformed before re-estimating the model. Finally, the fitted model is used to forecast future values of the time series.\\n\\nWhile being accurate, the primary limitation of the Box-Jenkins method lies in its inability to\\n\\nadapt to forecasting errors to enhance predictions. This drawback makes it challenging to apply\\n\\nin atypical system conditions. To address this limitation, Chen et al. introduced an adaptive\\n\\n2 approach. This method involves initially determining the error in the learning coefficients using\\n\\na minimum mean square error (MMSE), followed by updating forecasts using the one-step-ahead\\n\\nforecast error and the coefficients [14].\\n\\nFurthermore, Huang and Shih proposed an STLF model using ARMA considering non-Gaussian processes [15]. The method by Huang and Shih also proposes techniques to check the Gaussianity of the data. This is important as the data used for this work is not Gaussian.\\n\\nWhile ARMA models prove valuable in precise load forecasting, they fundamentally demand stationary data. For instance, the average load should remain consistent between morning and evening. Yet, in load forecasting, this doesn’t hold true, given that the required average load varies based on the time of day. Consequently, to handle non-stationarity within the load profile, an enhanced model called AutoRegressive Integrated Moving Average (ARIMA) is employed. The integrated component (I) involves differencing the data, making it stationary. This makes ARIMA a more robust forecasting technique for time series data with trends and seasonality, like power load profiles. Barakat [3] employed a seasonal ARIMA model to forecast load patterns encompassing seasonality from historical data. Juberias [16] designed a real-time load forecasting ARIMA model, integrating meteorological factors as explanatory variables. Furthermore, Cho et al. used an ARIMA model alongside a transfer function model to incorporate ambient temperature to increase the accuracy of the prediction [17]. The method by Cho et al. can be considered as the genesis of what is more commonly known as the ARIMAX modelling framework.\\n\\nThe ARIMAX model incorporates the eXogenous variables such as temperature, and time of day into the modelling. It can be considered as a form of multivariate ARIMA modelling, and is akin to moving from linear regression to multiple regression in Section 2.1.1. Furthermore, like ARIMAX, there also exists ARMAX for load forecasting, however, it suffers from the same issue as the base ARMA model of stationary in the data. The coefficients of the ARIMAX model are again estimated by a least squares approach. Nevertheless, Cui and Peng showcase the effect of using the ARIMAX model using the temperature as the eXogenous variable [4]. Cui and Peng also compared the mean absolute error (MAE) of the improved ARIMAX model with a simple ARMA process. They found that the ARIMAX approach has an MAE of 0.0037, whereas the ARMA(3,2) process has an MAE of 0.0494. Furthermore, Shipla and Sheshadri also performed the ARIMAX modelling using the weather as the eXogenous variable. They reported a mean absolute percentage error (MAPE) of 2.86% [18].\\n\\nWhile time series techniques are relatively accurate and can use multiple variables to foreacst load, these models assume linearity for the observed system. To address this constraint, non-linear\\n\\nmodels such as deep learning have been an active area of research.\\n\\nDeep Learning for Load Forecasting\\n\\nOne of the first approaches to load forecasting using deep learning was through feed-forwaed neural networks (FNN). The FNN architecture consists of three main components, the input layer, the hidden layers and the output layer. The depth of a network is given by the number of hidden layers, and the layer complexity is defined by the number of neurons per layer. FNNs only allow forward connections in the architecture and do not allow any recursion or feedback. Detailed theory on the implementation of FNNs can be readily found in the literature [19].\\n\\nThe initial approach for FNN in load forecasting used a single hidden layer. The authors of [20]\\n\\nuse a single layer FNN for daily forecasts using load and temperature data. However, arguably the\\n\\nvery first multi layered FNN for hourly load forecasting as developed by Srinivasan et al. in [21]. An MAE of 1.477% was reported for a weekday prediction using the multi layer FNN.\\n\\nSubsequently, in [22], the authors showcased the structure of an FNN and applied it to short term load forecast used 24 independent neural networks to predict the load for the next 24 hours. the authors decided to use the calendar day, weather variables and load data as three input vari- ables. A reported MAPE increase in the hourly load forecast of 30% and a 40% MAPE increase in the daily energy forecast were found.\\n\\nUsing FNNs was a great first step for deep learning in load forecasting. However, as mentioned previously, it is more than likely that the load data is dependent on previous values. This means that the data exhibits time dependencies, which cannot be exploited using simple FNNs. To capture such dependencies, a network architecture known as recurrent neural networks (RNNs) are used. The basic architecture for RNNs is shown in Figure 2.1.\\n\\nIn 2.1, xt represents a given time series sequence, hh denotes the hidden state and the estimated output is given by y^t. A detailed analysis of the underlying theory and mathematical formulation can be found in the literature [23]. However, in essence, RNNs are a type of artificial neural networks inherently designed to process sequences of data. Unlike traditional FNNs, which process each data point independently and don’t have memory of previous inputs, RNNs have a built- in memory mechanism that allows them to maintain information about previous inputs as they\\n\\nhinit\\n\\n2 xt h t x[0] h[0] [0]\\n\\nUNFLODING\\n\\nh[t-1]\\n\\nx[t] h[t] [t]\\n\\nFigure 2.1: The structure of an RNN featuring a single input. Revealing the internal workings demonstrates the inherent time-delay operation within RNNs. The design takes the form of a basic FNN, yet each layer is constrained to utilise the same weights, denoted as hinit.\\n\\nprocess the current input.\\n\\nThe fundamental building block of an RNN is the recurrent layer, which contains recurrent units (often represented as cells) that are connected to each other in a loop. Each recurrent unit takes an input, combines it with the previous hidden state (the memory from the previous step), and produces an output and a new hidden state. This hidden state acts as a representation of the network’s memory of previous inputs. The new hidden state is then fed back into the recurrent unit as the memory for the next step in the sequence. The basic update equation for an RNN is\\n\\nht = AF (Wxt + Uht 1 + b): (2.5)\\n\\nWhere AF is the activation function such as ReLU or tanh(), W is the current input weight matrix, U is the weight matrix for the previous state and b is a bias term.\\n\\nDue to RNNs’ inherent property for capturing time dependency in the data, they are extensively used in load forecasting. For example, the authors of [24] use a multi-scaled RNN model, which is able to model and capture both short-term and long-term temporal dependencies in the load data for one hour ahead forecasting. Their approach also showed an increased performance in forecasting when compared to ARIMA processes. Furthermore, in [25] the authors compared the performance and the feasibility of FNNs with backpropagation with the RNNs. In [25] seven different RNN architectures were implemented and tested. A 2% accuracy was reported for RNNs when compared to the FNNs with backpropagation. However, it was also noted that the training time for the FNN was about 1 minute and 30 seconds, whereas the RNN training was significantly longer at about 30 minutes.\\n\\nAlthough RNNs are great at capturing time series patterns where current values depend on the previous values, they suffer from vanishing/exploding gradients. A specific type of RNN is known as long-short term memory (LSTM).\\n\\nLSTMs address the challenge of the vanishing/exploding gradient problem encountered in con- ventional RNNs, as highlighted in [26]. Fundamentally, the intrinsic layout of an LSTM architecture mirrors that of a simple RNN. Yet, the pivotal distinction lies in LSTM’s utilisation of a gated mechanism that regulates neural information processing. Through these gated networks, gradient flow can be managed via gate values, effectively mitigating the vanishing gradient issue. This is achievable due to the LSTM’s capability to retain memory over extended durations. Load fore- casting involves predicting the future electricity consumption or demand based on historical data, weather patterns, and other relevant factors. LSTMs can capture long-term dependencies in data. The basic LSTM structure is shown in Figure 2.2.\\n\\nc[t-1] X c[t]\\n\\nf[t] X\\n\\ni[t] d[t]\\n\\no[t] X h[t]\\n\\nh[t-1]\\n\\nxt\\n\\nConcatenation Piece-wise Activation Inputs Outputs Operator Operations Functions\\n\\nFigure 2.2: LSTM diagram which shows the long term memory in the top horizontal line and the short term memory in the bottom horizontal line.\\n\\nFigure 2.2 represents the memory block with one cell. The quantities in blue represent the inputs to the cell and the ones in green represent the outputs. The inputs to the short term memory (bottom horizontal line) is the concatenation of the current time series value x[t], and\\n\\nthe previous hidden state h[t  1]. The short term memory consists of different layers or gates, all\\n\\ndriven by either  or a activation function. The output of the leftmost short term memory gate\\n\\n2 (also known as the forgetting gate) is f [t] and this is the percentage of the previous memory to\\n\\nremember. Since f [t] is a percentage value, the activation function  of the forgetting gate is the\\n\\nsigmoid function. This is because the dynamic range of the sigmoid activation function is from 0\\n\\nto 1.\\n\\nTo impose nonlinearity in the model, the concatenated h[t−1] and x[t] are parsed through a tanh activation giving d[t] which is responsible for updating the long term memory. The signal i[t] controls the influence of d[t] in updating the long term memory as it is the concatenated inputs parsed through the sigmoid activation. Likewise, the rightmost layer is responsible for updating\\n\\nthe the short term memory, and is used to determine the current hidden state.\\n\\nLSTMs have therefore been readily applied to load forecasting. In [27] the authors showcased\\n\\nthe use of LSTMs for one hour ahead predictions for the entire country of Poland compared with\\n\\na small area of Poland. The LSTM layers were trained under supervised learning and the adaptive movement estimation (ADAM) loss function. Additionally, Wang et al. implemented a LSTM model for load forecasting and compared it with a multilayered perceptron neural network (MLP), random forest (RF) and support vector machine (SVM). They found a reduction of 3% in the MAPE from the LSTM implementation as compared with the other aforementioned techniques. Many other LSTM implementations can be readily found in the literature [28][29].\\n\\nThe performance of the aforementioned deep learning models is dependent on the amount of data available. A common characteristic of the data for load forecasting is that it is distributed across the households. While hypothetically, the data from all the households can be combined to train a global model instead of only training on the locally available data, the aggregation of data is hindered due to logistical and privacy-related concerns. In this scenario, federated learning stands out as an effective solution as it enables collaborative training without the need to aggregate the data. The following subsection discusses the details about federated learning and its use for load forecasting as documented in the current literature.\\n\\nFederated Learning\\n\\nFederated Learning was first proposed by McMahan et al. in [5] and [30]. The core idea behind it is to collaboratively train a model amongst multiple clients which are independent of each other without uncovering their respective datasets. This allows clients to enjoy the benefits of having a more accurate model by leveraging the data from others without ever compromising privacy.\\n\\n2  The two most common training algorithms for federated learning are gradient descent (FedSGD) and federated averaging (FedAvg) [5]. Considering FedSGD, the clients first compute the gradients\\n\\nof the loss functions with respect to the weights using their local data. The loss function is a non- parametric function that penalises inaccurate predictions of our outputs of the model. The aim of the model is to minimise the value of this function.\\n\\nIt is known that the gradient of a function points in the direction of the steepest ascent of the function [31]. Thus, to minimise the loss function, small steps are taken in the opposite direction as to the gradient. These updated gradients are then transmitted to a central server. The server\\n\\nthen aggregates them in a weighted fashion where the weights are equal to the size of the dataset\\n\\nof the individual clients: w   PK nk g , where w is the weight at time t, K is the total\\n\\nt k=1 n k t\\n\\nnumber of houses indexed by k, is the learning rate, nk is the size of the dataset of client k, n is the size of all the datasets of individual clients combined, and gk is the gradient of client k.\\n\\nNow instead of each client taking only one step of gradient descent, multiple steps can be taken as well before the aggregating step. This approach is referred to as FederatedAveraging FederatedAveraging or FedAvg. The total computation needed is controlled using three important parameters: CC, the fraction of clients taking part in the computation in every round; EE, the number of steps taken by clients on their local datasets; and BB which is the local batch size employed for local client updates. It has been shown in[5] that FedAvg leads to a more accurate and faster convergence as a consequence of multiple local epochs as compared FedSGD.The pseudo- code for FedAvg can be seen in Algorithm 1 [5].\\n\\nApart from the different aggregation algorithms, federated learning also has different con- figurations dependent on the structuring of the available data. The difference is based on the configuration of the feature space X, the label space Y and the Identifiers (IDs) of the samples I . Thus different combinations of (X,Y, I ) are labelled as Horizontal FL, Vertical FL, Transfer FL\\n\\nand Assisted FL.\\n\\nHorizontal FL implies to the case when, say two clients i and j, have the same feature space, so Xi = Yi. Their respective label spaces are also the same as it is just the future load so\\n\\nYi = Yj . For the case of employing federated learning for STLF with various households, since the houses have same features (such as load values, weather. etc.) and same label (future forecast), Horizontal FL will be applicable.\\n\\nAlgorithm 1: Federated Averaging [5]\\n\\nData: K: Clients indexed by k, B: local minibatch size, E: number of local epochs, :\\n\\nlearning rate 2 Result: Global model wt\\n\\n1 Server executes:;\\n\\n2 Initialize w0;\\n\\n3 for each round t = 1;2;::: do\\n\\n4 m  max(C K; 1);\\n\\n5 St (random set of m clients);\\n\\n6 for each client k 2 St in parallel do\\n\\n7 wk;t+1 ClientUpdate(k;wt);\\n\\n8 mt nk;\\n\\n9 wt+1 mk12t SPt k2St nk wk;t+1;\\n\\n10 Function ClientUpdate(k;w):\\n\\n14 w  w    rmathcall(w;b);\\n\\n15 return w to server;\\n\\nVertical FL is applicable when the sample IDs are the same, so Ii = Ij , however both the feature spaces and the label spaces are different so Xi 6= Xj and Yi 6= Yj . An example of Vertical FL could be the case of a hospital and a pharmaceuticals company where the pharmaceuticals company wants to test a new drug they produced and thus they need to access the privacy-preserved data with the hospital about the medical and laboratory results relating to their drug.\\n\\nFederated Transfer Learning is the case when Xi 6= Xj , Yi 6= Yj and Ii 6= Ij . As a simple example of Transfer FL, consider a bank in India and retail business company in Germany, the intersection of users for these two companies is very small and the overlap between features\\n\\nis also small. In such a scenario Transfer FL is useful.\\n\\nApart from the aggregation methods and structural differences in data, federated learning can also be divided into two categories according to the presence or absence of a central server. In conventional FL, as explained above a central server overlooks the transmission, reception and the aggregation of the weights. However, this makes the system extremely dependent on only the server [32]. A failure at the server implies an absolute halt to the process. Thus to solve this problem a structure was proposed to send the gradients/weights directly to the other clients. This peer-to- peer federated learning is termed as Decentralized federated learning. While the synchronization effect is the same when the weights are sent by a client to all other clients, one major shortfall is\\n\\nthat it significantly increases the communications per round [32].\\n\\n2  The performance of a federated learning system is strongly dependent on the communication network topologies. In simple terms, this refers to the way the clients are connected to each other while sending their weights/gradients for aggregation. Kavalionak et al. [33] investigate the impact of different network topologies on the convergence of a decentralised federated learning system. They chose the following three topologies:\\n- Regular random graph: Every client (node) is randomly connected to a given number of clients (degree). Thus every client has the same number of neighbours.\\n- Small-World graph: Network constructed using the Watts-Strogatz method [34].\\n- Scale-free graph: Network built using the Barabási–Albert method [35].\\n\\nThey discovered that clustered networks such as small-world and scale-free lead to better perfor- mance. Small-scale network performs the best in the case of low training data per node. On the contrary, the Scale-free network outperforms the others in the case of a large-scale test. This indi- cates that hierarchical organization with some nodes having a higher degree (number of neighbours) than others leads to a higher convergence speed.\\n\\nAnother common network topology used for FL is the ring topology in which the clients are connected in a ring like fashion. Thus each client has only two neighbours. Do et al. [36] utilised a ring-based algorithm and were able to obtain an increase of 26% in the test accuracy.\\n\\nThe following subsection discusses the use of FL for the specific case of STLF.\\n\\nFederated Learning for Short-Term Load Forecasting\\n\\nThe use of federated learning for STLF is relatively new however the research is picking up due to the immense benefits gained by using FL. The initial studies use the FedSGD algorithm for FL [37], [38]. Apart from FedSGD, He et al. in [37] utilise k-means clustering with a different number of clusters to identify what number of clusters leads to the best performance. They discovered that clustering on the basis of different statistics of the load profile improves the results significantly. The Deep Learning architecture utilised by them is the LSTM. Lin et al. in [38] try to reduce the computational cost of FedSGD by introducing asynchronous stochastic gradient descent algorithm with delay computation (ASGD-DC). Their algorithm utilised the Taylor series expansion to compensate for the latency of clients having low computational strength.\\n\\nUnlike the aforementioned studies, the following researches employed FedAvg as opposed to\\n\\nFedSGD. Savi et al. in [39], Briggs et al [40], Biswal et al. [41] and Afaf et al. [42] analyse\\n\\n2 clustering as well similiar to He et al. [37]. It was discovered that k-means clustering on the socio-\\n\\neconomic demographic information can enhance the performance. All these studies utilise LSTMs\\n\\nas their deep learning model architecture. With certain conditions, their results also inform that\\n\\nit is feasible to train model with good accuracies with minimal number of clients as well.\\n\\nLi et al. [43] analyses the impact of the number of clients ranging from 2 to 6 on the overall performance. The number of total training rounds was also varied (5 - 15). The findings suggest that the overall accuracy enhancement is proportional to both the number of clients and the number of training iterations. Husnoo et al. [44] and Xu et al. [45] also analyse the effect of the number of participants on the final performance. The findings are interesting as they discovered that increasing the number of clients leads to a decrease in accuracy. This was attributed to the non-IID nature of the data of different clients. The architecture utilised for these two studies was also LSTM.\\n\\nConsidering the comparison of FedSGD to FedSGD for STLF, Fekri et al. [46] showed that FedAvg outperforms FedSGD. This shows the reproducibility of the claim in [5]. Shi et al. [47] utilises a non-canonical algorithm for aggregation. They use a modified version of maximum mean discrepancies (MK-MMD) with multiple kernels to optimize the global model with the help of tuning. They obtained superior results as compared to the results with FedAvg.\\n\\nWhile the above studies have thoroughly investigated the use of FL for STLF, none of them have analysed the performance of different network topologies for the specific case of STLF. Furthermore, considering fairness, although they have suggested techniques such as local fine-tuning, none of them investigate which clients are not benefitting if it is feasible to predict these non-benefitting clients beforehand. A non-benefitting client is someone who attains no enhancement in their performance after training using the federated setup as compared to when the model is solely trained on their local dataset. Thus, the aim of this study is to investigate these two facets.\\n\\nCHAPTER 2. LITERATURE REVIEW\\n\\nMethodology\\n\\nContents\\n\\n**Dataset: Description and Processing . . . . . . . . . . . . . . . . . . . . 19\\n\\n**Deep Learning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\n**Federated Learning Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n\\n**Network Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\n**Evaluation Methods and Metrics . . . . . . . . . . . . . . . . . . . . . . . 25\\n\\nEvaluation of Network Topologies . . . . . . . . . . . . . . . . . . . . . . 27\\n\\nEvaluating the Feasibility of Predicting Non-Benefitting Clients . . . . . 27\\n\\nThis chapter aims to provide the reader with the relevant methods for setting up the exper- iments and evaluating the results. Firstly, information about the utilised dataset is presented involving some basic statistical measures describing the data distribution. This is followed by the discussion about the federated learning set-up employed. Subsequently, the architecture for the deep learning model used for STLF is discussed and the values for various hyperparameters are presented. Penultimately, the used network topologies and evaluation metrics are mentioned. Finally, the methods to predict the non-benefitting clients are discussed.\\n\\nDataset: Description and Processing\\n\\nThe dataset utilised for this study is the smart-meter data provided by the energy company, UK Power Networks [48]. An augmented version of this data which includes the values for additional variables such as temperature and demographics is obtained from Jean-Michel D.’s Kaggle contri- bution [49].\\n\\nThe dataset consists of the electricity consumption readings for every half-hour interval for 5567 households in London. The household selection was done carefully to ensure a balanced representation of households in the Greater London Area. These households took part in a Low Carbon London project initiated by UK Power Networks from November 2011 to February 2014.\\n\\n3  The data consists of two user groups. The first subgroup was subjected to Dynamic Time of Use (dTOU) prices for the year 2013. This implied that these households were informed of the\\n\\nprices a day in advance. They were informed of the variable rates of electricity applied and also at what times of the day these prices are applicable. This offers the customers an opportunity for monetary benefit by using electricity when the rates are lower. The second subgroup was subjected to a flat tariff rate. To reduce variability, only this second group was considered. This leads to a reduction in the total number of households from 5567 to 4443\\n\\nOf the rest of the households, only those households were considered that had readings for the entirety of 2013. This further reduced the total number of households to 4039. Missing values were imputed with the average of the readings one hour prior and later. Finally, the values were aggregated to obtain a coarser granularity of one hour.\\n\\nSince federated learning performs better when the data is homogenous across various clients [50], k-means clustering, with k = 18, was performed on the basis of different metrics related to the load profiles of the households as done by Savi and Olidavese in [39]. They showed that independent models for each cluster lead to better performance as compared to the same without clustering. The features used for clustering were as follows:\\n\\nMedian electricity consumption\\n\\nAverage electricity consumption\\n\\nTotal electricity consumption\\n\\nWeekday corresponding to the most average electricity consumption\\n\\nWeekday corresponding to the least average electricity consumption\\n\\nHighest electricity consumption\\n\\nLowest electricity consumption\\n\\nHaving performed the clustering, the cluster with the maximum number of clients was selected (298). Out of these, a random set of 32 clients were selected for the federated learning setup. This\\n\\nDEEP LEARNING MODEL 21\\n\\nFigure 3.1: Example of the load profile for an arbitrary household for a period of 2 days.\\n\\nnumber was chosen while keeping the computational speed and capacity in mind. An example of the load value for an arbitrary household for three days can be seen in Fig. 3.1\\n\\nDeep Learning Model\\n\\nThe inspiration for the deep learning model is taken from [39]:\\n\\nThe first layer, also known as the input layer, consists of the load value for the past 24 hours.\\n\\nThe second layer consists of 32 LSTM units. The activation function for the LSTMs is selected to be the hyperbolic tangent (tanh) activation.\\n\\nThe third layer consists of 16 LSTM units. For this layer as well, the LSTMs have tanh as their activation function.\\n\\nThe fourth and the final layer consist of a single neuron with no activation function.\\n\\nIn addition to this, a regularisation strategy called Dropout was employed to prevent overfitting. Dropout involves arbitrarily leaving out (dropping) some neurons during both, the forward and the backward propagation preventing overfitting. A Dropout layer was added before each of the two LSTM layers. The reader is advised to review [51] for further details on Dropout.\\n\\nFurthermore, another regularisation strategy called Early Stopping was incorporated as well. It refers to halting the training process once the loss has not improved on the validation set even after a predecided number of epochs (patience). The patience was chosen to be 10 for the process.\\n\\nThe following are the features used for the deep learning model after taking inspiration from\\n\\n[39]:\\n\\nLoad value for the last 24 hours.\\n\\nWeekday encoded as a number with Monday being 0.\\n\\nHour of the day encoded as a number from 0 to 24.\\n\\n3 • AVG4D: This feature is the average value for the load consumed for the last 4 days. It is noteworthy that the last four days are split into two categories: weekday and weekend. Thus,\\n\\nif the current day is a weekday, the last four days will be the last for weekdays, thus skipping any weekend days if they happen to lie in the window of the last four days. The same holds for weekend days.\\n\\nTempCluster: This feature encompasses the effect of apparent temperature. Apparent temper- ature is the temperature calculation after accounting for external factors such as humidity, wind speed etc. [52]. TempCluster clusters the apparent temperature into two groups: 0 and\\n\\n1. This level of granularity was found to be sufficient to incorporate the effect of temperature [39].\\n\\nThe loss function was chosen to be the Mean Absolute Error (MAE):\\n\\n1 Xn\\n\\nMAE = n jyi   y^ij; (3.1)\\n\\ni=1\\n\\nwhere yi is the true load value and y^i is the estimated load value. The reason for choosing MAE over Mean Squared Error (MSE) as a loss function is the robustness of MAE to outliers.\\n\\nThe training plus the validation set included the readings from the month of January through November. The split between the training and validation was 90 to 10. The readings for the month of December were used for the test set.\\n\\nFederated Learning Setup\\n\\nThe federated setup is facilitated by a central server that performs the aggregation of weights for each household according to their connectivity with others based on the chosen network topology. The chronology for the setup is as follows:\\n\\nStep 1: The server initialises the model weights.\\n\\nStep 2: The server distributes these model weights to the local models for each household.\\n\\nFEDERATED LEARNING SETUP 23\\n\\nStep 3: Each household takes a predecided number of steps, n, of gradient descent on their local dataset.\\n\\nStep 4: The updated model weights are sent back to the server.\\n\\nStep 5: The weights are aggregated by the server according to the network topology. The 3 aggregation is a weighted average where the weights for averaging are the size of the local\\n\\ndatasets.\\n\\nStep 6: Repeat from step 2 until convergence.\\n\\nThis process is also illustrated in Fig. 3.2. The elaborate algorithm for the FL training procedure\\n\\nWeights Initialisation\\n\\nServer 5. Weights 4. Weights sent\\n\\naggregation to the server\\n\\nWeights sent\\n\\nto the households\\n\\nHousehold Household Household\\n\\nLocal weights 3. Local weights 3. Local weights update update update\\n\\nFigure 3.2: Federated Learning setup\\n\\nutilised can be seen in Algorithm 2. The inspiration has been taken from [39] and the necessary modifications for the study have been performed.\\n\\nTable 3.1 summarises the values of the hyperparameters and other details regarding the deep learning model and the federated learning setup such as the size of the training set, number of local epochs, etc.\\n\\nEVALUATION METHODS AND METRICS\\n\\n[^1]Algorithm 2: Federated Learning Training Procedure: inspired from [39]\\n\\n1 Nround: number of rounds (t being its index)\\n\\n2 wj;t: weight j of the global model at round t\\n\\n3 wi : weight j of the local model of household i at round t\\n\\nj;t\\n\\n4 ni: number of samples of household i\\n|Name|\\n    Value|\\n| - | - |\\n|\\n    Training samples per household|\\n    7192|\\n|\\n    Validation samples per household|\\n    800|\\n|\\n    Test samples per household|\\n    743|\\n|\\n    Number of local epochs (Nepoch)|\\n    5|\\n|\\n    Number of total rounds (Nround)|\\n    50|\\n|\\n    Total number of households|\\n    32|\\n|\\n    Batch Size (Bsize )|\\n    100|\\n|\\n    Optimisation Algorithm|\\n    Adam|\\n|\\n    Learning Rate (LR)|\\n    0.0001|\\n|\\n    Dropout|\\n    10%|\\n|\\n    Early stop patience|\\n    10|\\n\\n3** [^2] Nepoch: number of local epochs\\n\\n6 Bsize: local batch size\\n\\n7 LR: learning rate for the Adam optimiser\\n\\n8 Sni : Set of households which are connected to household i. 9 for round t = 1 to Nround do\\n\\n10 Send wj;t to the local households\\n\\n11 Local Households:\\n\\n12 for each Household i 2 S do\\n\\nfor each epoch l = 1 to N do\\n\\n13 for each batch bi = 1 toepochni do\\n\\n14 ri Gradient computedBsizeusing Adam\\n\\n1516 wjil;t wji;t   LRedge  ril 8j\\n\\n17 Send wi 8j to Aggregator\\n\\nj;t\\n\\n18 AggregatorP: nFedAV Gn\\n\\n19 wj;t+1 i2Si ~~P~~ ni~~ wji;t 8j\\n\\nn ^i2S^ni n^i\\n\\nEVALUATION METHODS AND METRICS 25\\n\\n1) Ring Graph\\n\\n2) Small-Scale Graph (Wattz- Strogatz)\\n\\n3) Scale-Free Graph (Barabási- Albert)\\n\\nEVALUATION METHODS AND METRICS 29\\n|||\\n| :- | :- |\\n|||\\n|||\\n|||\\n4) Fully-Connected Graph\\n\\nFigure 3.3: Arbitrary examples of the four network topologies. The blue circles are the nodes which refer to the households and the edges are used to connect the households which are sharing weights with each other.\\n\\nthe edges. The values of k and p are arbitrarily set to 4 and 0.3 respectively.\\n\\nScale-free graph: Network built using the Barabási-Albert technique [35]. It is parame- terised using m which refers to the number of nodes with edges connecting to other nodes with a high degree.\\n\\nFully-Connected graph: All nodes are connected to each other, sometimes also referred to as star topology.\\n\\nThe network types are listed in decreasing order of sparsity with the Ring graph being the most sparse and the Fully-connected graph being the least. For visual appreciation, an example of these network types can be seen in Fig. 3.3.\\n\\n3.5 Evaluation Methods and Metrics\\n\\nThe advantage of employing federated learning becomes evident when we compare the prediction accuracy of a household that has participated in the federated setup with the accuracy achieved by the household when the model was solely trained using its own local data. To evaluate the accuracy of the models, the mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean square error (RMSE) evaluation metrics are used, given as (3.2), (3.3) and (3.4)\\n\\n3 respectively MAE = n1 X jy^   yij; (3.2) n\\n\\ni=1\\n\\n1 Xn y^i   yi\\n\\nMAPE = i=1 yi  100%; (3.3)\\n\\nand\\n\\nvuu 1 Xn\\n\\nRMSE = t (y^\\n\\nn i   yi)2 (3.4)\\n\\ni=1\\n\\nobtained from [53] and [54]. In (3.2), (3.3), and (3.4), n is the total number of samples, yi is the true load, and y^i is the estimated load.\\n\\nRMSE is the most inappropriate evaluation metric for load forecasting because it gives prece- dence to larger errors and is more prone to volatility in the presence of outliers. MAE on the other\\n\\nhand, is less prone to outliers, however, both MAE and RMSE are scale-dependent metrics. This is illustrated with a simple example. Consider a forecasting scenario where the mean load is 10 kWh\\n\\nand another scenario where the mean load is 0.1 kWh. Now, if MAE is 0.1 for both these scenarios,\\n\\none might be mistaken to think that the model is equally accurate for both cases. However, as a percentage, 0.1 is 1% of 10 and 100% of 0.1. This shows that the latter case is much worse than\\n\\nthe former case, thus exhibiting the scale dependency of the metric MAE. The same argument can be made about RMSE as well. The only downside to MAPE is its vulnerability to bursting if the\\n\\ntrue load values yi are close to zero. This can be seen from (3.3), where a smaller value of yi will give a large value for MAPE. However, as seen from Figure (3.1) the smallest load value is roughly 0.15 kWh, which is relatively larger than zero. This makes MAPE relatively superior to MAE and RMSE for load forecasting and therefore, it will be given precedence for evaluating improvement between the federated scenario and the local data training scenario.\\n\\nEvaluation of Network Topologies\\n\\nTo evaluate the improvement between using the federated setup and training the model solely on\\n\\nthe house’s local data, a MAPE comparison is made between the two modeling approaches for all\\n\\n3 four network topologies in Fig. 3.3. The number of clients benefiting between the two modeling\\n\\napproaches is defined by counting the households that attain a lower MAPE in the federated setup\\n\\nthan the MAPE from the model trained solely on the house’s local data. In addition to this,\\n\\nthe MAPE, MSE, MAE, and their respective improvements using the federated setup were also\\n\\ncalculated. Improvement is defined as a reduction in these error metrics. Mathematically, the\\n\\nimprovement for a metric m is calculated as follows:\\n\\nmimp = mlocal   mmodel ; (3.5) where mimp is the improvement for metric m which can be MAE, MAPE, or RMSE, mlocal is\\n\\nthe metric’s value using the localised model and mmodel is the metric’s value using another model, either a federated model or the centralised model. Thus improvement for a metric is synonymous with a reduction for that error metric.\\n\\nFor an additional comparison, a model is trained for the hypothetical case of aggregating the data from all the available households. This is called the centralised model. This model is then tested on the local datasets of each individual household and the aforementioned metrics are calculated. The results corresponding to this centralised model serve as the ideal or optimal results as this is the best-case scenario but is practically infeasible. All experiments were repeated with different random seeds to analyse generalisability.\\n\\nIn addition to analysing the loss metrics, the convergence speed was also analysed by plotting the learning curves which refer to the plot that shows the reduction in the validation loss as the learning progresses.\\n\\nEvaluating the Feasibility of Predicting Non-Benefitting Clients\\n\\nFeature Vector Construction\\n\\nAs mentioned earlier, the second facet of this project is to predict of the non-benefitting clients beforehand. To do so, the following different statistics of the load profile of various houses are calculated:\\n\\nMean of the load consumed for the kth house (l;k ): The mean value captures the centrality of the load consumption pattern of a household.\\n\\nMedian of the load for the kth house (ml;k ): Since the mean is affected by outliers, the use\\n3  of another central metric like median which is not affected by the outliers is beneficial.\\n\\nStandard deviation for the kth house (l;k ): This measures the variation of the load consumed on an average from the mean. This metric captures the range of variation.\\n\\nInter Quartile Range for the kth house (IQRl;k ): This is the difference between the 75th and the 25th percentile. This metric is similar to the standard deviation in the sense that it is a measure of the variation of the load values for a household. However, instead of focusing on the mean, the focus is on the median making it robust to the outliers.\\n\\n]T. These four values form a feature vector for each house k denoted by fk : [l;k ;ml;k ;l;k ;IQRl;k\\n\\nSince the non-benefiting clients for a given set of clients are expected to be the ones whose data diverges from the data of their neighbours significantly, the identification of these clients depends on the dataset of the other clients. Thus, the predictive algorithm will need the feature vectors of all the clients combined which increases the input dimensionality significantly. An alternative to this is that the feature vectors can be normalised to obtain relative numbers to ensure that the predictive algorithm learns to use the deviation of only the given household’s vector from a central value and does not require all clients’ vectors as the input for predictive purposes. To ensure this, Z-score normalisation is applied as follows:\\n\\nf n = fk   f ; (3.6)\\n\\nk f\\n\\nwhere fkn is the normalised vector, f is the mean of the feature vectors for a given set of clients and f is the standard deviation for a given set of clients. This normalisation focuses on the relative distance of the vectors from the central value for the vectors for a given set of clients. This ensures that when these vectors are fed into the predictive model, it learns to only utilise relative divergence. Since only a few statistical measures are required to calculate these feature vectors, privacy related to the exact load consumption pattern is still preserved.\\n\\nFor visual appreciation, Principal Component Analysis [55] is applied to project these vectors onto a 3D and 2D plane to analyse if there is a clear distinction between the benefitting and non-benefitting clients.\\n\\nClassification Techniques\\n\\nOne of the most fundamental machine learning algorithms that can be utilised for classification is\\n\\nthe Decision Tree [56]. The construction of these trees is based on dividing the data into smaller\\n\\nsubsets dependent on the most informative input features recursively. The nodes of the trees are 3 decisions, the branches leading out are the results and the resulting leaf nodes are the class labels.\\n\\nThe leaf nodes are then split in a similar manner until the obtained leaf nodes are pure. Purity\\n\\nrefers to the point when one lead node only consists of the labels of one class. To measure the\\n\\nlevel of impurity or the amount of mixing of classes, Gini [57] or entropy is calculated [56]. This\\n\\nleads to a tree-like structure, hence the name, decision trees. Although decision trees are easy to\\n\\ninterpret, they are susceptible to overfitting especially when the tree grows deep. To overcome this\\n\\nproblem, ensemble techniques have been proposed.\\n\\nOne such technique is Bootstrap Aggregating or Bagging for short [58]. Bagging utilises the concept of bootstrapping which refers to the selection of random samples from the training dataset with replacement. After bootstrapping, different decision trees are trained on the different boot- strapped samples. This causes different trees to learn from different subsets of the training data. For the final predictions, the predictions from these individual trees are combined, which is usually using majority voting. However, this final combination strategy is dependent on the amount of data samples for each class in the dataset and the penalty for misclassifying different classes. Bag- ging reduces variance and thus increases the generalisability of the decision tree and thus decreases overfitting. Another advantage of bagging is that the individual trees can be trained in parallel thus improving overall performance without significantly adding to the training time.\\n\\nAlthough bagging is a very impactful ensemble learning method that increases the accuracy of the final predictions, there are some drawbacks. Since for each of the individual trees involved in bagging, all the features are utilised for the output, there is high variance. This can also be problematic when the input features are correlated with each other strongly. Furthermore, bagging fails to provide any important metric for the features. This makes the process of interpreting the model more difficult. These challenges are solved by an alternative ensemble method called a random forest.\\n\\nRandom forest can be thought of as an extension of bagging [59]. It also relies on the idea of aggregating the output from multiple trees to produce the final output however it introduces a feature selection method that ensures that the trees are uncorrelated from each other to maximise the gain of incorporating different trees. Instead of considering all the features at every node during\\n\\nEVALUATION METHODS AND METRICS\\n\\nFigure 3.4: A visual representation of an SVM. The circles and triangles denote points from two distinct classes respectively. The black line is the hyperplane of separation. The bordering purple lines are the margins that pass through the two closest points called support vectors which define the margins of maximum separation.\\n\\nFigure 3.5: A visual representation of a ran- dom forest. The predictions from the indi- vidual trees are aggregated, often using max voting, to produce the final result. For the case of regression, averaging is used but for our case of identifying benefitting clients, only the classifier use of the random forest is relevant.\\n\\nEVALUATION METHODS AND METRICS 33\\n\\nthe process of building a tree, a random sampling of features is done to only access a subset of the features for a tree. This sampling technique is responsible for uncorrelated different trees as there is a high chance that they end up selecting different features. Furthermore, random forests can also compute feature importances to identify if any feature is redundant. This feature importance metric also helps with interpretability.\\n\\nAnother machine learning tool widely used for classification is the Support Vector Machine or SVM [60]. Unlike Random Forest, SVM constructs a hyperplane that separates the two classes in the best way possible by maximising the distance/margin between them. One of the most important characteristics of SVM that makes it very powerful is the ability to use kernel function. Kernels allow the transformation of the input feature space into a space of much higher dimensionality. This allows the SVM to capture complex relationships, thus inherently SVM is able to capture more complex patterns. Two of the most common kernels used are the polynomial kernel and the Radial Basis Function (RBF) [60].\\n\\nHowever, its worth mentioning that SVM is computationally more expensive. Furthermore, the use of kernels with SVM makes it less interpretable. In addition to this, careful fine-tuning of hyperparameters may be necessary for attaining sufficient accuracy.\\n\\nFigure 3.5 and 3.4 illustrate a visual representation of a random forest and an SVM respectively. For the purpose of this project, all three of the aforementioned techniques: Bagging, random forest, and SVM were used for prediction, and their results were compared to select the best.\\n\\nBefore proceeding to the subsequent part, the definitions for some necessary terms are provided\\n\\nbelow:\\n\\nTrue Positive (TP): The model predicts the outcome as true when the actual outcome is\\n\\n3 true as well.\\n\\nTrue Negative (TN): The model predicts the outcome as false when the actual outcome is true.\\n\\nFalse Positive (FP ): The model predicts the outcome as true when the actual outcome is false.\\n\\nFalse Negative (FN ): The model predicts the outcome as false when the actual outcome is false as well.\\n\\nPrecision: This metric measures how accurately the model can identify TP from all the outcomes labeled as True. It is a metric that measures the quality of predictions. It is calculated using the equation:\\n\\nTP\\n\\nPrecision =~~ : (3.7)\\n\\nTP + FP\\n\\nRecall: This metric measures how many TP are captured by the model out of the entire set of Positives. This is a metric that measures the quantity of true predictions. It is calculated using the equation:\\n\\nTP\\n\\nRecall = (3.8)\\n\\nTP + FN\\n\\nTrue Positive Rate (TPR) or Sensitivity: This measures the relevance of the items selected. It is calculated using the equation: TPR = TP . This is the same as recall.\\n\\nTP +FN\\n\\nFalse Positive Rate (FPR) or Inverted Specificity (1 - Specificity): This measures that out of all the outcomes labeled as false, which ones are actually false. It is calculated using the equation:\\n\\nFP\\n\\nFPR = (3.9)\\n\\nFP + TN\\n\\nBalanced Accuracy: For imbalanced datasets, conventional metrics like accuracy are not informative they often attain high value solely by predicting the majority class all the time. In such a case, balanced accuracy provides a better measure of performance by equally\\n\\nweighing the prediction of positive and negative classes [61]. The formula is as follows:\\n\\nTP + FP\\n\\nBalanced Accuracy = (Sensitivity + Specificity)/2 = TP +FN FP +TN (3.10)\\n\\n3 It is worth mentioning that we choose the label 1 or true if a client is not benefitting and 0 or false if they are benefitting. It is expected that the number of non-benefitting clients is going to\\n\\nbe fewer than the benefitting clients. This results in an extremely imbalanced dataset which can hamper the performance of these classification models. Furthermore, it is more harmful to include\\n\\na client who is not going to benefit as opposed to removing a client who will benefit. Thus, in our case, false negatives (FNs) are more harmful than false positives (FPs). Thus we prefer recall over precision as recall encompasses FN (3.8) while precision does not (3.7).\\n\\nTo address this issue of an imbalanced dataset the following strategies are employed:\\n\\nBagging with random undersampling: A simple way to ensure equal representation of classes in the bootstrapped dataset is to randomly undersample the majority class to attain a desired ratio between the two classes (usually 1).\\n\\nRandom forest with undersampling: Similiar to bagging with undersampling, in this case as well, the majority class is randomly undersampled to get a balanced bootstrapped sample.\\n\\nRandom forest with class weights: As mentioned earlier, the impurity score using Gini or entropy is calculated to determine the split of each node in the tree. By default, this cal- culation usually involves setting equal weights for all the classes involved. However, these weights can be adjusted to allow for some mixing in favour of either of the classes. The weights are chosen to be inversely proportional to the frequency of occurrence of the class in the bootstrapped sample. This allows the weights to be in favour of the minority class, which in our case is the class of non-benefitting clients. The penalises FP more than FN.\\n\\nSVM with class weights: Similar to the random forest with class weights, for SVM as well the weights can be adjusted to be inversely proportional to the frequency of occurrence of the classes.\\n\\nTwo of the most common visual evaluations of the discriminatory power of a model are the receiver operating characteristic (ROC) curve and the precision-recall curve. The ROC curve plots the TPR on the x-axis and the 1- FPR on the y-axis for different threshold values. A model with no skill results in equal roughly equal TPR and FPR for a given threshold. Thus a no-skill\\n\\nclassifier has a line with a slope of 1 on its ROC curve. A skillful classifier is able to maximise TPR\\n\\nand minimise FPR. Thus for a skillful classifier, the ROC curve bows to the left. The threshold\\n\\ncorresponding to the leftmost point is deemed as the best as it leads to the maximum TPR and\\n\\nFPR. The ROC curve can allow us to choose a different threshold than the default value of 0.5 to\\n\\n3 suit the needs of the specific problem. An example of a ROC curve can be seen in Fig. 3.6.\\n\\nFigure 3.6: An example of a ROC curve Figure 3.7: An example of a precision- for a skillful and a no-skill classifier. recall curve for a skillful and a no-skill The area under the curve (AUC) is classifier. The area under the curve a way to quantify the discriminatory (AUC) is a way to quantify the discrim- power of the model on an average across inatory power of the model on an aver- all thresholds. age across all thresholds.\\n\\nA drawback of the ROC curve is that it is not suitable for imbalanced datasets, especially when the false class (0) is much bigger than the true class (1). In such a case, the model can just learn to predict the majority class (0 in this case) most of the time. Due to the fact that the the number\\n\\nof observations of class 0 is much higher, this would lead to very high TN, which in turn would lead to low FPR (3.9) giving an illusion that the ROC curve has high performance. The use of the precision-recall curve can offer a solution to this problem. Since we are less invested in observing the skill of the model to predict 0 and more interested in observing the accuracy of predicting 1, we focus on precision (3.7) and recall (3.8) as both of them do not consider TN but focus on TP. Furthermore, as explained earlier, in the case of predicting non-benefitting clients, since FNs are more harmful than FPs, we can vary the threshold to gain a higher recall (as recall considers FN (3.8) at the cost of some manageable reduction in precision (3.7) if required. It is also worth noting that the area under the curve (AUC) for the ROC or the precision-recall curve is the quantification of the distinguishing power of the model across all thresholds.\\n\\nThus for all the tree-based methods and SVM described earlier, the precision, recall, AUC-ROC, AUC-precision-recall, and balanced accuracy are calculated to choose the best model.\\n\\nLastly, as mentioned before, random forests enable the calculation of feature importances. This\\n\\n35 CHAPTER 3. METHODOLOGY\\n\\ncan be very useful as these values can observed to analyse which feature in the input vector is the most useful for the classification of the benefitting from the non-benefitting clients. This analysis can also inform us about any redundant feature corresponding to a significantly lower value than others. Thus, the feature importances for the random forest are calculated as well.\\n\\n3  The next section presents and discusses the results of the aforementioned experiments.\\n\\n4  4\\n\\nResults and Discussion\\n\\nContents\\n\\n**Evaluation of network topologies . . . . . . . . . . . . . . . . . . . . . . . 35\\n\\nLoss Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n\\nConvergence time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n\\n**Predicting Non-Benefitting Clients . . . . . . . . . . . . . . . . . . . . . . 42\\n\\nEvaluation of network topologies\\n\\nLoss Metrics\\n\\nThe results for the number of clients benefitting can be seen in Fig. 4.1 for six different random seeds which corresponds to six different sets of 32 clients. The experiment was repeated with six seeds to ensure that the findings were generalisable. The definition of benefitting clients can be reviewed from Chapter 3. It is worth noticing that the ring topology consistently results in the highest number of benefitting clients. What is particularly interesting is that for some seeds (seeds 3 and 4), the ring topology not only performs the best compared to the other topologies but also results in an equal number or higher than the number of households benefitting from the centralised model. This could be attributed to the fact that the data of the individual houses is heterogeneous and the federated scenario allows for the models to be tailored to the unique characteristics of the houses. This might not be the case with the centralised model as it is not able to generalise enough to capture the unique traits of all houses.\\n\\nEVALUATION OF NETWORK TOPOLOGIES 37\\n\\nConsidering the other 3 topologies, there does not seem to be a clear ranking. For instance, for seed 2, the fully connected network is the second best followed by small-scale and scale-free. However, if you consider seed 5, small-scale is the second best followed by scale-free and fully connected.\\n\\n||Fully Connected||Scale-Free||Small-Scale||Ring Centralised|\\n| :- | - | :- | - | :- | - | :- | - |\\n\\n30\\n\\n25 4\\n\\n20 15 10 5 0\\n\\n1 2 3 4 5 6\\n\\nRandom Seed\\n\\nFigure 4.1: The number of benefitting clients for different network topologies for six random sets of 32 clients corresponding to six random seeds.\\n\\nThe mean MAPE improvement for the benefitting clients for the four network topologies along with the one achieved using the centralised model can be seen in Fig. 4.2. The higher the MAPE improvement, the better a model is performing. It is noteworthy that the MAPE improvement is comparable for the four network topologies for most seeds except for some exceptions such as seed 1 and 2 for which the fully-connected and the ring topology significantly outperform the scale- free and the small-scale model. Additionally, while the MAPE improvement did not surpass the improvement using the centralised model, the values are very close for some seeds (seeds 3 and 6) suggesting that in some cases, the effectiveness of federated learning approaches the performance of the ideal centralised scenario.\\n\\nThe MAPE values for the 32 clients for the four network topologies along with the ones achieved using the centralised and the local models can be seen in Fig. 4.3. The lower this mean MAPE value, the better the model’s performance on average. It can be seen that the four network topologies result in lower MAPE compared to the MAPE with the localised model for all the seeds except seed one for which all topologies expect ring result in a higher mean MAPE. This can be explained by observing the number of benefitting clients for seed one in Fig.4.1. Since the number of benefitting clients is very low for fully-connected, scale-free, and small-scale topologies, the MAPE improvement for the benefitting clients is offset by the increase in MAPE for the non-benefitting\\n\\nEVALUATION OF NETWORK TOPOLOGIES\\n\\nFully Connected Scale-Free Small-Scale Ring Centralised\\n\\n20 15 10 5 0\\n\\n1 2 3 4 5 6 Random Seed\\n\\nFigure 4.2: MAPE improvement (3.5) for the four network topologies and the cen- tralised model for the benefitting clients for six random sets of 32 clients corresponding to six random seeds.\\n\\nFully Connected Scale-Free Small-Scale Ring Centralised Local\\n\\n70 60 50 40 30 20 10 0\\n\\n1 2 3 4 5 6 Random Seed\\n\\nFigure 4.3: Mean MAPE value for the four\\n\\nnetwork topologies, the centralised model, 4 and the localised model for six random sets\\n\\nof 32 clients corresponding to six random\\n\\nseeds.\\n\\nEVALUATION OF NETWORK TOPOLOGIES\\n\\nclients. This explains the higher mean MAPE corresponding to these network topologies.\\n\\nThe mean MAE improvement for the benefitting clients for the four network topologies along with the one achieved using the centralised model can be seen in Fig. 4.4. The MAE improvement does not seem to be following any regular pattern considering the network topologies however, the ring topology leads to either the most or the second most improvement in MAE as can be seen from the plot.\\n\\nEVALUATION OF NETWORK TOPOLOGIES 39\\n\\nFully Connected Scale-Free Small-Scale Ring Centralised 0.03\\n\\n0.025\\n\\n0.02\\n\\n0.015\\n\\n0.01\\n\\n0.005\\n\\n1 2 3 4 5 6 Random Seed\\n\\nFigure 4.4: MAE improvement (3.5) for the four network topologies and the centralised model for the benefitting clients for six ran- dom sets of 32 clients corresponding to six random seeds.\\n\\nFully Connected Scale-Free Small-Scale Ring Centralised Local 0.14\\n\\n0.12\\n\\n0.1\\n\\n0.08\\n\\n0.06\\n\\n0.04\\n\\n0.02\\n\\n1 2 3 4 5 6 Random Seed\\n\\nFigure 4.5: Mean MAE value for the four network topologies, the centralised model, and the localised model for six random sets of 32 clients corresponding to six random seeds.\\n\\nEVALUATION OF NETWORK TOPOLOGIES\\n\\nThe mean MAE values for all the network topologies, the centralised and the local model are illustrated in Fig. 4.4. It is noteworthy that the MAE for all the network topologies is lower than the one attained using the localised model for all seeds except 1. This is expected due to the fact that the benefitting clients for all the network topologies except for the ring lead to the low number of benefitting clients as explained earlier. It is interesting to observe that ring topology consistently leads to the lowest MAE for all seeds.\\n\\nFigure 4.6 illustrates the RMSE improvement for all topologies and the centralised model. The\\n\\nobservation is similar to that with MAE improvements. There isn’t an inherent pattern except for the fact that the ring topology leads to an improvement that is either the best or the second best among all topologies.\\n\\nThe mean RMSE for all the topologies, the centralised and the local model can be seen in Fig. 4.7. Similar to MAE, for RMSE as well the network topologies lead to a lower mean MAPE except\\n\\n4 for seed one due to the low number of benefitting clients as explained earlier. It is noteworthy that the ring topology leads to the lowest RMSE for all seeds.\\n\\nEVALUATION OF NETWORK TOPOLOGIES\\n\\nFully Connected Scale-Free Small-Scale Ring Centralised\\n\\n0.035 0.03 0.025 0.02 0.015 0.01 0.005 0\\n\\n|||||||||||||||||\\n| :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- |\\n|||||||||||||||||\\n|||||||||||||||||\\n|||||||||||||||||\\n|||||||||||||||||\\n|||||||||||||||||\\n|||||||||||||||||\\n1 2 3 4 5 6 Random Seed\\n\\nFigure 4.6: RMSE improvement        RMSE improvement (3.5) for the four    network topologies and the centralised    model for the benefitting clients for six random sets of 32 clients corresponding to six random seeds.\\n\\nFul\\n\\nly Connected Scale-Free Small-Scale Ring Centralised Local\\n\\n0.25 0.2 0.15 0.1 0.05 0\\n\\n1 2 3 4 5 6 Random Seed\\n\\nFigure 4.7: Mean RMSE\\n\\nMean RMSE value for the four network topologies, the centralised model, and the\\n\\nlocalised model for six random sets of 32 clients corresponding to six random seeds.\\n\\nEVALUATION OF NETWORK TOPOLOGIES 43\\n\\nThe average improvements in MAPE, MAE, and RMSE can be seen in Table 4.1. It can be observed that on average, the ring topology leads to the most amount of improvement for all three metrics: MAPE, MAE, and RMSE. In addition to this, it also leads to the most number of benefitting clients which establishes its superiority over the other network topologies.\\n\\nTable 4.1: Average MAPE, MAE, and RMSE improvements for six seeds for the four network topologies\\n\\n|Model|Number of benefitting clients|\\n\\nMAPE\\n\\nimp|\\n\\nMAE\\n\\nimp|\\n\\nRMSE\\n\\nimp|\\n| - | - | - | - | - |\\n|Fully-Connected|21.5|9.83%|0.0137|0.0153|\\n|Scale-Free|20.67|8.77 %|0.013|0.0149|\\n|Small-Scale|21.83|9.84%|0.0144|0.0163|\\n|Ring|25.5|10.58%|0.0183|0.0224|\\n\\nQuantitatively, the analysis of the performances of different network topologies can be done using graph theory. We can construct an adjacency matrix (A) [62] of a given graph for which,\\n\\nAi;j = 1 if nodes/clients i and j are connected otherwise Ai;j = 0. By definition, A is a square matrix of size n which is equal to the total number of nodes/clients involved. An example adjacency\\n\\nmatrix is as follows: 2 3\\n\\n666601 10 17777 : 1\\n\\nA =\\n\\n4 5\\n\\n1 1 0\\n\\nAnother important matrix characterising a given network is the degree matrix, D [62]. It is\\n\\nalso a square matrix of size n with entries only along the diagonals. A given entry Di;i is equal to\\n\\n4 the number of edges (degree) emanating from node i which tells us about the number of nodes a\\n\\ngiven node i is connected to. The matrix D corresponding to the A above is as follows:\\n\\n2 3 62 0 07\\n\\n6 7\\n\\nD = 60 2 07 :\\n\\n6 7\\n\\n4 5\\n\\n0 0 2\\n\\nIt can be seen that the entries for D are just the sum across the rows or columns of A.\\n\\nFrom these two matrices, another matrix called the Laplacian matrix, L, can be calculated as the difference between D and A. It can provide very useful information about the convergence and stability of the network [62]. The Laplacian corresponding to the A and D above is as follows:\\n\\n2 3 666 2  21   117777 :\\n\\nL = D   A = 6 1\\n\\n4 5  1  1 2\\n\\nThe information regarding how a given matrix transforms a vector is encapsulated in its eigen-        values [62]. It has been discovered that the closer the eigenvalue of L is to 0, the more robust is the network to noise or data perturbations. The eigenvalues corresponding to the four networks\\n\\ncan be seen in Fig. 4.8.\\n\\nIt can be seen that the closeness of the eigenvalues to zero is directly proportional to the sparsity of the topology with the ring topology having the values closest to 0 as it is the most sparse and vice-versa for fully connected. This makes the ring topology most stable to data perturbations which are a result of data heterogeneity for our case of STLF.\\n\\nQualitatively we can say that the reason that ring topology leads to the best performance can be attributed to data heterogeneity. Since this topology is relatively sparse, the lower number of connections implies a smaller amount of data distribution via gradient sharing between clients.\\n\\nFigure 4.8: Eigenvalues corresponding to the Laplacian of the four network topologies. The total number of eigenvalues is equal to the number of nodes (32 for our case). All the eigenvalues for fully connected are equal to each other except for the first one. This is why it is observed that only two red points are visible as all the points are superimposed.\\n\\nConsequently, the effect of others on a given client is lower, leading to smoother gradient steps and smaller deviations from the local gradients. This allows a client to capture enough information from others while maintaining vicinity to its local load characteristics. This is extremely beneficial as ring topology also has the lowest number of communication rounds per episode (N) as compared to the more denser networks such as fully connected (N 2). This allows us to attain better results while reducing the number of communication rounds which are very expensive considering limited bandwidth in real-world scenarios.\\n\\nThus, it can be said that for heterogeneous datasets, sparser topologies like the ring topology lead to better overall performance.\\n\\nConvergence time\\n\\nApart from analysing the loss metrics, it is also important to look at the convergence profile. The learning curves for the four network topologies for the 32 clients corresponding to the three random seeds can be seen in Fig. 4.9, Fig 4.10, and Fig 4.11 respectively.\\n\\nIt is noteworthy that the ring topology takes the longest number of epochs to converge. This is as expected due to the fact the ring topology has the smallest number of connections, thus the information diffusion takes time as it has to pass through multiple clients which takes multiple epochs.\\n\\nFigure 4.9: Learning curve (seed 2) Figure 4.10: Learning curve (seed 3)      The validation loss vs. epoch for all the four The validation loss vs. epoch for all the four network topologies for the 32 clients network topologies for the 32 clients      corresponding to seed 1 corresponding to seed 2\\n\\nFigure 4.11: Learning curve (seed 3)      The validation loss vs. epoch for all the four network topologies for the 32 clients      corresponding to seed 3\\n\\nMathematically, the convergence time can also be calculated using the Laplacian. The second smallest eigenvalue of L is known as the Fiedler eigenvalue [62]. It quantifies the rate of information transfer and hence the speed of convergence. The higher the Fiedler eigenvalue, the higher the rate of information diffusion and hence speed of convergence. Figure 4.12 shows the Fiedler eigenvalue for the four network topologies. It can be seen that the Fiedler eigenvalue is the highest for fully connected and decreases as the sparsity increases to the smallest for the ring topology. This accurately reflects in the experimentally obtained results for some cases such as in Fig. 4.11, where fully connected converges the fastest followed by scale-free, small-scale, and ring (decreasing order of density). However, this is not always the case as this theoretical formulation considers data homogeneity which is not true for our case of STLF. This deviation from the theory can be seen for instance in Fig. 4.10, where scale-free and small-scale converge faster than fully connected due\\n\\nPREDICTING NON-BENEFITTING CLIENTS 45\\n\\nFigure 4.12: The Fiedler eigenvalue of the Laplacian for the four network topologies. to data heterogeneity.\\n\\nThe reader might wonder why the curves for network topologies halt even though they seem to be decreasing (for example scale-free in Fig. 4.9). This is due to the fact that the early stopping is applied at an individual level but the plot shows the average MAE for all the clients. Thus, it is possible that for a subset of clients, the validation loss has not improved even after the decided threshold of epochs (10 in our case) and thus the federated procedure was halted even though the average loss might be decreasing giving an illusion of non-optimal stopping.\\n\\nPredicting Non-Benefitting Clients\\n\\nThis section presents and discusses the results of various prediction models used to predict non- benefitting clients.\\n\\nFor visual appreciation, the feature vectors fk for the houses were projected onto a 3-D and a 2-D. This was done to visually analyse for any clear separation between the benefitting and the non-benefitting clients.\\n\\nFigures 4.13 and 4.14 illustrate the projections of the clients’ feature vectors onto the 3-D and\\n\\nthe 2-D space respectively.\\n\\nPREDICTING NON-BENEFITTING CLIENTS\\n\\nFigure 4.13: PCA 3D\\n\\nThe PCA projection of the feature vectors of the houses on the 3D space.\\n\\nFigure 4.14: PCA 2D\\n\\nThe PCA projection of the feature vectors of the houses on the 2D space.\\n\\nPREDICTING NON-BENEFITTING CLIENTS\\n\\nIt can be seen that there does not seem to be any clear distinction between the benefitting and the non-benefitting clients. This suggests that the task of distinguishing benefitting from non-benefitting clients cannot be carried out by just performing basic statistical tests using the load values of the houses. This observation inspires the experimentation of machine learning classification models to capture the underlying complex relationship between the statistics of the load profile and whether a house is going to benefit or not.\\n\\nBefore proceeding, it is important to analyse which clients are not benefitting from using the four network topologies. Table 4.2 below presents this result.\\n\\n|Network Topology|IDs of the non-benefitting clients|\\n| - | - |\\n|Fully-connected|22, 24, 25, 26, 31|\\n|Scale-free|2, 3, 9, 12, 16, 19, 22, 24, 25, 26, 31|\\n|Small-scale|9, 12, 13, 19, 22, 24, 25, 31|\\n|Ring|19, 22, 24, 31|\\n\\nTable 4.2: The list of IDs for the non-benefitting clients for a set of 32 clients corresponding to seed 2\\n\\nIt is very interesting to see that there is a great degree of overlap between the sets of non- benefitting clients for different network topologies. For example, the IDs 19, 22, 24, and 31 cor- responding to the ring also appear as scale-free and small-scale. Even for fully connected, all the IDs except 19 appear. This result was observed when repeated with different sets of 32 clients establishing its generalisability. This suggests that the non-benefitting clients are not affected by the network topology used for federated learning.\\n\\nThus it was decided to solely use the non-benefitting clients using the ring topology as it lead to the highest number of benefitting clients. The training data consists of the non-benefitting clients for 13 sets of 32 clients. The trained model was then tested on an unseen set of 32 clients.\\n\\nAs a recap, the following methods are being used for classification:\\n\\nBagging with random undersampling\\n\\nRandom forest with random undersampling\\n\\nRandom forest with class weights\\n\\nSVM.\\n\\nFor all the tree-based classification methods mentioned above, 800 trees were used. A forest or a bagging classifier has another parameter: the maximum depth of the trees involved. This is a regularisation parameter as smaller depths lead to less overfitting but extremely small depths can also lead to underfitting. After thorough experimentation, the optimal value for this parameter was set to 4. The kernel for SVM was set to RBF as that led to the best results.\\n\\nAs explained in the methodology section, in our case, FNs are more dangerous than FPs. Thus, whenever there is a trade-off between recall and precision, a higher recall is prioritised as recall encompasses FNs (3.8). To choose the best threshold considering the precision and recall, the precision-recall curve is drawn for all four methods and the appropriate threshold is chosen.\\n\\nIt was discovered that random forest with class weights led to the best performance followed by SVM with class weights. Figure 4.15 and 4.16 depict the precision-recall curve and the ROC curve for random forest with class weights respectively. The chosen threshold along with the corresponding metric values have been annotated. It can be seen that commendable values of recall and precision are obtained.\\n\\nPREDICTING NON-BENEFITTING CLIENTS\\n\\nFigure 4.15: The precision-recall curve for the random forest with class weights\\n\\nFigure 4.16: The ROC curve for the random forest with class weights\\n\\nPREDICTING NON-BENEFITTING CLIENTS\\n\\nThe figures 4.17 and 4.18 depict the precision-recall and the ROC curve. It can be observed that although the recall is similar to the one obtained using the random forest, the recall is much lower.\\n\\nPREDICTING NON-BENEFITTING CLIENTS 47\\n\\nFigure 4.17: The precision-recall curve for the random forest with class weights\\n\\nFigure 4.18: The ROC curve for the random 4 forest with class weights\\n\\nPREDICTING NON-BENEFITTING CLIENTS\\n\\nTable 4.3: Performance metrics for all four classification models.\\n\\n|Metrics|Random forest with class weights|SVM with class weights|Bagging with undersampling|Random forest with undersampling|\\n| - | :-: | :- | - | :-: |\\n|Precision|0.82|0.8|0.643|0.6|\\n|Recall|0.82|0.73|0.82|0.82|\\n|AUC-ROC|0.9|0.887|0.887|0.81|\\n|AUC Precision-Recall|0.834|0.832|0.826|0.829|\\n|Balanced Accuracy|0.86|0.816|0.79|0.766|\\n|False Positive Rate (FPR)|0.095|0.096|0.238|0.286|\\n|True Positive Rate (TPR)|0.82|0.73|0.82|0.82|\\n\\nTable 4.3 shows the values for all the metrics for all four methods. It can be seen the random forest with class weights leads to the best values for all values further establishing its superiority.\\n\\nThe reason for its supremacy is attributed to the combination of handling complex patterns, preventing overfitting due to ensembling, and representing the imbalanced class using the appro- priate weights. SVM has a lower performance as its kernel-based calculations might be affected by the imbalanced class due to the absence of a clear margin. The undersampling methods are considerably inferior as although undersampling might help with balancing the dataset, it often leads to loss of information.\\n\\nHaving observed the random forest with class weights, it is important to analyse which feature contributes the most to the outcome by calculating the feature importance. The values can be seen in Table 4.4. It can be seen that the median has a higher value than the mean implying that it is\\n\\nTable 4.4: Feature importances for the random forest with class weights\\n\\n|Feature|Feature importance value|\\n| - | - |\\n|Mean l|0.217|\\n|Median ml|0.44|\\n|Standard deviation l|0.169|\\n|Inter-quartile range IQRl|0.174|\\n\\nCHAPTER 4. RESULTS AND DISCUSSION\\n\\npreferred as the central tendency over the mean. This is understandable as the mean is affected significantly by outliers. In terms of the measures for variation, the weights for the standard deviation and IQR are relatively similiar implying that both capture the variation with equivalent accuracy with the former capturing the entire range and the latter gathering information about a closer interval around the median.\\n\\nConclusions\\n\\nConclusion\\n\\nIn conclusion, the topic of short-term load forecasting (STLF) was introduced and its importance was discussed by understanding the pivotal role played by it in maintaining grid stability by balancing supply and demand. The conventional methods used for load forecasting and their shortcoming which ultimately led to the adoption of deep learning methods like LSTM were also presented. Moreover, the essential foundations for understanding these models were also provided. Subsequently, it was also discussed that the accuracy of these deep learning models is highly dependent on the amount of data available which is often low for STLF as the data is distributed amongst households. Aggregating this data into one place was found to be an inviable option due to logistical and privacy reasons.\\n\\nAs a solution to this problem, the paradigm of federated learning was introduced which enables collaborative training without the need of aggregating the data in one place, hence, solving the aforementioned problems. Subsequently, it was discussed that federated learning is significantly affected by the way the clients are connected to each other. Furthermore, after taking part in a federated learning scenario, not everybody benefits thus fairness is an issue. To date, nobody has investigated the effect of network topology on federated learning for STLF. Furthermore, no analysis has been done about the non-benefitting clients and if it is feasible to predict them beforehand. Thus, these two analyses were selected as the core facets of this study.\\n\\nConsidering the network topologies, it was discovered that the ring topology led to the best results considering the number of clients who benefitted (25.5), and the reduction in the loss metrics: MAPE, MAE, and RMSE on average. This was mathematically scrutinized by looking at the eigenvalues of the Laplacian of the ring topology graph. The values were much closer to 0 signifying more stability to data perturbations due to data heterogeneity. In terms of convergence time, however, it was discovered that the ring topology took the longest. This was in line with the theory which said that the Fiedler eigenvalue is inversely proportional to the convergence time and it was calculated to be the smallest for the ring topology.\\n\\n48 CONCLUSIONS\\n\\nTo identify the non-befitting clients, their feature vectors containing information on the statis- tics of their load profiles were calculated and normalised. For visual analysis, these vectors were projected onto the 3D and 2D space using PCA. No distinction could be observed. For predicting these clients, bagging with undersampling, random forest with undersampling, random forest with class weights, and SVM with class weights were investigated. Precision recall curves were drawn for each method to determine the optimal threshold. It was discovered that the random forest with class weights led to the best performance giving a balanced accuracy of 86%, a precision of 82%, and a recall of 82% as well followed by svm with class weights. This was attributed to the fact that ensemble methods prevent overfitting and class weights allow us to reduce data imbalance proficiently unlike the undersampling methods which reduced the imbalance but led to informa- tion loss. The reason for random forest outperforming SVM was attributed to the difficulty faced by the SVM using kernel methods to draw margins for the minority class. In conclusion, it was demonstrated that it is feasible to identify the non-benefitting clients with good accuracy.\\n\\nFuture Work\\n\\nFor future work, the following aspects can be studied considering this project\\n\\nThe effect of choosing a different deep learning model such as transformers on the results obtained. This can tell us if the results are model agnostic or not.\\n\\nThe effect of utilising a different federated aggregation function such as FedProx [6] that deals with data heterogeneity.\\n\\nTrying a hybrid network topology that combines multiple methods such as the ring topology with the fully connected one by having a subset of clients connected in a ring-like fashion and connecting these clusters in a fully connected manner and vice-versa.\\n\\nRemoving the non-benefitting clients and analysing the change in the proportion of clients benefitting.\\n\\nBibliography\\n\\nA. D. Papalexopoulos and T. C. Hesterberg, “A RECRESSION-BASED APPROACH TO SHORT-TERM SYSTEM LOAD FORECASTING,” Tech. Rep., 1989.\\n\\nG. Mbamalu and M. El-Hawary, “Load forecasting via suboptimal seasonal autoregressive models and iteratively reweighted least squares estimation,” IEEE Transactions on Power Systems, vol. 8, no. 1, pp. 343–348, 1993.\\n\\nE. Barakat, M. Qayyum, M. Hamed, and S. Al Rashed, “Short-term peak demand forecasting in fast developing utility with inherit dynamic load characteristics. i. application of classical time-series methods. ii. improved modelling of system dynamic load characteristics,” IEEE Transactions on Power Systems, vol. 5, no. 3, pp. 813–824, 1990.\\n\\nH. Cui, X. Peng, et al., “Short-term city electric load forecasting with considering temperature effects: An improved arimax model,” Mathematical Problems in Engineering, vol. 2015, 2015.\\n\\nB. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication- efficient learning of deep networks from decentralized data,” in Artificial intelligence and statistics, PMLR, 2017, pp. 1273–1282.\\n\\nP. Kairouz, H. B. McMahan, B. Avent, et al., “Advances and open problems in federated learning,” Foundations and Trends® in Machine Learning, vol. 14, no. 1–2, pp. 1–210, 2021.\\n\\nE. Almeshaiei and H. Soltan, “A methodology for Electric Power Load Forecasting,” Alexan- dria Engineering Journal, vol. 50, no. 2, pp. 137–144, 2011, issn: 11100168. doi: 10.1016/ j.aej.2011.01.015.\\n\\nInstitute of Electrical and Electronics Engineers. and I. Motilal Nehru National Institute of Technology (Allahabad, ICPCES 2012 : 2012 2nd International Conference on Power, Control and Embedded Systems, 17th-19th Dec, 2012, Motilal Nehru National Institute of Technology Allahabad, India, isbn: 9781467310499.\\n\\nG. Gross and F. D. Galiana, “Short-term load forecasting,” Proceedings of the IEEE, vol. 75, no. 12, pp. 1558–1573, 1987.\\n\\nQ. Ding, “Long-term load forecast using decision tree method,” in 2006 IEEE PES Power Systems Conference and Exposition, IEEE, 2006, pp. 1541–1543.\\n\\nBIBLIOGRAPHY 55\\n\\nN. Amral, C. Ozveren, and D. King, “Short term load forecasting using multiple linear regression,” in 2007 42nd International universities power engineering conference, IEEE, 2007, pp. 1192–1198.\\n\\nK. Liu, S. Subbarayan, R. Shoults, et al., “Comparison of very short-term load forecasting techniques,” IEEE Transactions on power systems, vol. 11, no. 2, pp. 877–882, 1996.\\n\\nE. Barakat, J. Al-Qassim, and S. Al Rashed, “New model for peak demand forecasting applied to highly complex load characteristics of a fast developing area,” in IEE Proceedings C (Generation, Transmission and Distribution), IET, vol. 139, 1992, pp. 136–140.\\n\\nJ.-F. Chen, W.-M. Wang, and C.-M. Huang, “Analysis of an adaptive time-series autoregres- sive moving-average (arma) model for short-term load forecasting,” Electric Power Systems Research, vol. 34, no. 3, pp. 187–196, 1995.\\n\\nS. J. Huang and K. R. Shih, “Short-term load forecasting via ARMA model identification in- cluding non-Gaussian process considerations,” IEEE Transactions on Power Systems, vol. 18, no. 2, pp. 673–679, May 2003, issn: 08858950. doi: 10.1109/TPWRS.2003.811010.\\n\\nG. Juberias, R. Yunta, J. G. Moreno, and C. Mendivil, “A new arima model for hourly load forecasting,” in 1999 IEEE Transmission and Distribution Conference (Cat. No. 99CH36333), IEEE, vol. 1, 1999, pp. 314–319.\\n\\nM. Cho, J. Hwang, and C. Chen, “Customer short term load forecasting by using arima trans- fer function model,” in Proceedings 1995 International Conference on Energy Management and Power Delivery EMPD’95, IEEE, vol. 1, 1995, pp. 317–322.\\n\\nG. Shilpa and G. Sheshadri, “Arimax model for short-term electrical load forecasting,” Int.\\n\\nRecent Technol. Eng.(IJRTE), vol. 8, no. 4, 2019.\\nM. Denuit, D. Hainaut, and J. Trufin, “Feed-forward neural networks,” Springer Actuarial, 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID:209072018.\\nD. C. Park, M. El-Sharkawi, R. Marks, L. Atlas, and M. Damborg, “Electric load forecasting using an artificial neural network,” IEEE transactions on Power Systems, vol. 6, no. 2, pp. 442–449, 1991.\\nD. Srinivasan, A. Liew, and C. Chang, “A neural network short-term load forecaster,” Electric Power Systems Research, vol. 28, no. 3, pp. 227–234, 1994.\\nS. S. Sharif and J. H. Taylor, “Short-term load forecasting by feed forward neural networks,” in Proc. IEEE ASME First Internal Energy Conference (IEC), Al Ain, United Arab Emirate, 2000.\\nL. R. Medsker and L. Jain, “Recurrent neural networks,” Design and Applications, vol. 5, no. 64-67, p. 2, 2001.\\nV. Bui, T. L. Pham, J. Kim, Y. M. Jang, et al., “Rnn-based deep learning for one-hour ahead load forecasting,” in 2020 International conference on artificial intelligence in information and communication (ICAIIC), IEEE, 2020, pp. 587–589.\\nJ. Mandal, A. Sinha, and G. Parthasarathy, “Application of recurrent neural network for short term load forecasting in electric power system,” in Proceedings of ICNN’95-International Conference on Neural Networks, IEEE, vol. 5, 1995, pp. 2694–2698.\\nS. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\\nT. Ciechulski and S. Osowski, “High precision lstm model for short-time load forecasting in power systems,” Energies, vol. 14, no. 11, p. 2983, 2021.\\nY. Jin, H. Guo, J. Wang, and A. Song, “A hybrid system based on lstm for short-term power load forecasting,” Energies, vol. 13, no. 23, p. 6241, 2020.\\nM. S. Hossain and H. Mahmood, “Short-term load forecasting using an lstm neural network,” in 2020 IEEE Power and Energy Conference at Illinois (PECI), IEEE, 2020, pp. 1–6.\\nJ. Konečnỳ, H. B. McMahan, D. Ramage, and P. Richtárik, “Federated optimization: Dis- tributed machine learning for on-device intelligence,” arXiv preprint arXiv:1610.02527, 2016.\\nR. Brereton, “1.18 - steepest ascent, steepest descent, and gradient methods,” in Comprehen- sive Chemometrics, S. D. Brown, R. Tauler, and B. Walczak, Eds., Oxford: Elsevier, 2009, pp. 577–590, isbn: 978-0-444-52701-1. doi: https://doi.org/10.1016/B978-044452701- 1.00037-5. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ B9780444527011000375.\\nC. Hu, J. Jiang, and Z. Wang, “Decentralized federated learning: A segmented gossip ap- proach,” arXiv preprint arXiv:1908.07782, 2019.\\nH. Kavalionak, E. Carlini, P. Dazzi, L. Ferrucci, M. Mordacchini, and M. Coppola, “Impact of network topology on the convergence of decentralized federated learning systems,” in 2021 IEEE Symposium on Computers and Communications (ISCC), 2021, pp. 1–6. doi: 10.1109/ISCC53001.2021.9631460.\\nA. Barrat and M. Weigt, “On the properties of small-world network models,” The European Physical Journal B-Condensed Matter and Complex Systems, vol. 13, pp. 547–560, 2000.\\nR. Albert and A.-L. Barabási, “Statistical mechanics of complex networks,” Reviews of mod- ern physics, vol. 74, no. 1, p. 47, 2002.\\nJ.-w. Lee, J. Oh, S. Lim, S.-Y. Yun, and J.-G. Lee, “Tornadoaggregate: Accurate and scalable federated learning via the ring-based architecture,” arXiv preprint arXiv:2012.03214, 2020.\\nY. He, F. Luo, G. Ranzi, and W. Kong, “Short-term residential load forecasting based on fed- erated learning and load clustering,” Oct. 2021, pp. 77–82. doi: 10.1109/SmartGridComm51999. 2021.9632314.\\nJ. D. Fernández, S. P. Menci, C. M. Lee, A. Rieger, and G. Fridgen, “Privacy-preserving feder- ated learning for residential short-term load forecasting,” Applied energy, vol. 326, p. 119915, 2022.\\nM. Savi and F. Olivadese, “Short-Term Energy Consumption Forecasting at the Edge: A Federated Learning Approach,” IEEE Access, vol. 9, pp. 95949–95969, 2021, issn: 21693536. doi: 10.1109/ACCESS.2021.3094089.\\nC. Briggs, Z. Fan, and P. Andras, “Federated learning for short-term residential load fore- casting,” arXiv preprint arXiv:2105.13325, 2021.\\nM. Biswal, A. S. M. Tayeen, and S. Misra, “Ami-fml: A privacy-preserving federated machine learning framework for ami,” arXiv preprint arXiv:2109.05666, 2021.\\nA. Taı̈k and S. Cherkaoui, “Electrical load forecasting using edge computing and federated learning,” in ICC 2020-2020 IEEE international conference on communications (ICC), IEEE, 2020, pp. 1–6.\\nJ. Li, Y. Ren, S. Fang, K. Li, and M. Sun, “Federated learning-based ultra-short term load forecasting in power internet of things,” Aug. 2020, pp. 63–68. doi: 10.1109/ICEI49372. 2020.00020.\\nM. A. Husnoo, A. Anwar, N. Hosseinzadeh, S. N. Islam, A. N. Mahmood, and R. Doss, “Fedrep: Towards horizontal federated load forecasting for retail energy providers,” in 2022 IEEE PES 14th Asia-Pacific Power and Energy Engineering Conference (APPEEC), IEEE, 2022, pp. 1–6.\\nY. Xu, C. Jiang, Z. Zheng, B. Yang, and N. Zhu, “Lstm short-term residential load forecasting based on federated learning,” in 2021 international conference on mechanical, aerospace and automotive engineering, 2021, pp. 217–221.\\nM. N. Fekri, K. Grolinger, and S. Mir, “Distributed load forecasting using smart meter data: Federated learning with recurrent neural networks,” International Journal of Electrical Power\\nEnergy Systems, vol. 137, p. 107669, 2022.\\nY. Shi and X. Xu, “Deep federated adaptation: An adaptative residential load forecasting approach with federated learning,” Sensors, vol. 22, no. 9, 2022, issn: 1424-8220. doi: 10. 3390/s22093264. [Online]. Available: https://www.mdpi.com/1424-8220/22/9/3264.\\nSmart meter energy consumption data in london households, 2018. [Online]. Available: https: //data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households.\\nJ.-M. D, Smart meters in london, 2018. [Online]. Available: https://www.kaggle.com/ datasets/jeanmidev/smart-meters-in-london.\\nD. Gao, X. Yao, and Q. Yang, “A survey on heterogeneous federated learning,” arXiv preprint arXiv:2210.04505, 2022.\\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” Journal of Machine Learning Research, vol. 15, no. 56, pp. 1929–1958, 2014. [Online]. Available: http://jmlr.org/ papers/v15/srivastava14a.html.\\nR. G. Steadman, “A universal scale of apparent temperature,” Journal of Applied Meteorology and Climatology, vol. 23, no. 12, pp. 1674–1687, 1984.\\n\\nZ. Wang, J. Li, S. Zhu, et al., “A review of load forecasting of the distributed energy system,” in IOP Conference Series: Earth and Environmental Science, IOP Publishing, vol. 237, 2019,\\n\\n\\n042019.\\n\\nJ.-F. Chen and Q. H. Do, “Forecasting daily electricity load by wavelet neural networks optimized by cuckoo search algorithm,” in 2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI), IEEE, 2017, pp. 835–840.\\nS. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,” Chemometrics and Intelligent Laboratory Systems, vol. 2, no. 1, pp. 37–52, 1987, Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists, issn: 0169-7439. doi: https://doi. org/10.1016/0169-7439(87)80084-9. [Online]. Available: https://www.sciencedirect. com/science/article/pii/0169743987800849.\\nA. Navada, A. N. Ansari, S. Patil, and B. A. Sonkamble, “Overview of use of decision tree algorithms in machine learning,” in 2011 IEEE Control and System Graduate Research Col- loquium, 2011, pp. 37–42. doi: 10.1109/ICSGRC.2011.5991826.\\nJ. Grabmeier and L. A. Lambe, “Decision trees for binary classification variables grow equally with the gini impurity measure and pearson’s chi-square test,” Int. J. Bus. Intell. Data Min., vol. 2, pp. 213–226, 2007. [Online]. Available: https://api.semanticscholar.org/ CorpusID:41174210.\\nM. Skurichina and R. P. Duin, “Bagging for linear classifiers,” Pattern Recognition, vol. 31, no. 7, pp. 909–930, 1998.\\nL. Breiman, “Random forests,” Machine learning, vol. 45, pp. 5–32, 2001.\\nY. Zhang, “Support vector machine classification algorithm and its application,” in Inter- national Conference on Information Computing and Applications, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:21762245.\\nV. Garcı́a, R. A. Mollineda, and J. S. Sánchez, “Index of balanced accuracy: A performance measure for skewed class distributions,” in Iberian conference on pattern recognition and image analysis, Springer, 2009, pp. 441–448.\\nP. Zhu and R. C. Wilson, “Stability of the eigenvalues of graphs,” in International Conference on Computer Analysis of Images and Patterns, Springer, 2005, pp. 371–378.\\n\\n[^1]: Table 3.1: Summary of the hyperparameters and settings utilised for the federated deep learning setup.\\n[^2]: \\n    3.4 Network Topologies\\n\\nRing graph: This network refers to the nodes connected in a circular ring-like fashion. Each node has two neighbours in this configuration.\\n\\nSmall-world graph: Network built using the Watts-Strogatz method [34]. It is parame- terised by k which refers to the nearest neighbours and p which is the probability of rewiring', metadata={'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.md'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name='angad_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=angad_embedding)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# texts = text_splitter.split_documents(pages)\n",
    "# documents = [i.page_content for i in texts]\n",
    "# metadatas = [i.metadata for i in texts]\n",
    "# ids = [str(i) for i in range(len(documents))]\n",
    "# collection.add(\n",
    "# documents = documents,\n",
    "# metadatas = metadatas,\n",
    "# ids = ids\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12 CHAPTER 1. INTRODUCTION\\nas ambient weather and time of day. To incorporate such variables, multivariate linear regression\\nknown as multiple regression was used [2]. However, load data has inherent temporal which are\\nnot exploited using regression techniques. ARIMA (AutoRegressive Integrated Moving Average)\\nmethods have shown increased performance accuracies for load forecasting compared to regression\\n[3]. The multivariant version of ARIMA known as ARIMAX incorporates the external factors into\\nthe forecasting [4]. A thorough investigation into the statistical methods is conducted in Chapter\\n2. These techniques assume linearity, but deep learning offers better forecasting accuracy through\\nnon-linear modeling.\\nDeep learning is a branch of machine learning that uses neural networks with multiple layers\\nto automatically learn and extract complex patterns and representations from data, enabling the\\ndevelopment of advanced AI systems capable of tasks like image recognition and natural language\\nunderstanding. Deep learning is capable of capturing both temporal and spatial patterns in the\\nload data enabling more dynamic load forecasting.\\nHowever, load forecasting relies on historical load profiles, this data may contain personal\\nidentifiable information about clients. Therefore, load forecasting is inherently a privacy issue.\\nEven if data is anonymised it may still be possible to identify individuals from external data\\nsources (data leakage). In the interest of client privacy and security, a branch of deep learning\\nknown as federated learning has recently been used for load forecasting.\\nFederated learning is a decentralised machine learning approach where a global model is trained\\ncollaboratively across multiple decentralised devices or servers, allowing data to remain local while\\nstillbenefitingfromcollectivemodelimprovements. Themainmethodforthisremotecollaboration\\nis the averaging of the weights received from multiple clients during the gradient descent procedure.\\nThis is done using a specific aggregation algorithm such as Federated Averaging (FedAVG) [5].\\nA prominent advantage of federated learning is that it aligns with the decentralised nature of\\nelectricity grids, as data sources are distributed across different regions and stakeholders. Accurate\\nand robust forecasting models can be made using federated learning to access the distributed data.\\nAnother advantage is that it promotes collaboration among utilities and grid operators enabling\\nfor more accurate forecasting accuracy without the risk of sharing proprietary information.\\nA very important characteristic of federated learning is the way the clients are connected to\\neach other for weight sharing. This is parametrised using a network topology. Federated learning\\nis significantly affected by this network topology. Furthermore, it has been observed that fairness\\nis an open challenge as everybody does not benefit by taking part in federated learning process'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get(0)['documents'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Federated Learning for Short-Term\\nLoad Forecasting\\nAuthor\\nA. Khurana\\nCID: 02298815\\nSupervised by\\nDr Fei Teng\\nA Thesis submitted in fulfillment of requirements for the degree of\\nMaster of Science in Applied Machine Learning\\nDepartment of Electrical and Electronic Engineering\\nImperial College London\\n2023', metadata={'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.pdf', 'page': 0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [i.page_content for i in pages]\n",
    "metadatas = [i.metadata for i in pages]\n",
    "ids = [str(i) for i in range(len(documents))]\n",
    "collection.add(\n",
    "documents = documents,\n",
    "metadatas = metadatas,\n",
    "ids = ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who is my supervisor?\"\n",
    "question = \"\"\n",
    "results = collection.query(query_texts = [question], n_results = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['20', '34', '3', '7', '5']],\n",
       " 'distances': [[1.2122215032577515,\n",
       "   1.3455324172973633,\n",
       "   1.387810230255127,\n",
       "   1.5093117952346802,\n",
       "   1.6102261543273926]],\n",
       " 'metadatas': [[{'page': 21,\n",
       "    'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.pdf'},\n",
       "   {'page': 35,\n",
       "    'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.pdf'},\n",
       "   {'page': 4,\n",
       "    'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.pdf'},\n",
       "   {'page': 8,\n",
       "    'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.pdf'},\n",
       "   {'page': 6,\n",
       "    'source': 'C:\\\\Users\\\\angad\\\\OneDrive\\\\Desktop\\\\AML\\\\thesis\\\\MSc_Thesis_02298815.pdf'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['14 CHAPTER 1. INTRODUCTION',\n",
       "   '218 CHAPTER 2. LITERATURE REVIEW',\n",
       "   'Declaration of Originality\\nI hereby declare that the work presented in this thesis is my own unless otherwise stated. To the\\nbest of my knowledge the work is original and ideas developed in collaboration with others have\\nbeen appropriately referenced.',\n",
       "   'Acknowledgments\\nI would firstly like to thank my supervisor Dr Fei Teng for encouraging thought provoking discus-\\nsions and providing incomparbale guidance throughout this thesis. I am also very thankful for Mr\\nXu’s for valuable inputs and insights on this thesis.\\nFinally, I am very grateful to my friends and family for continuously supporting me throughout\\nthis degree.',\n",
       "   'Copyright Declaration\\nThe copyright of this thesis rests with the author and is made available under a Creative Commons\\nAttribution Non-Commercial No Derivatives licence. Researchers are free to copy, distribute or\\ntransmit the thesis on the condition that they attribute it, that they do not use it for commercial\\npurposes and that they do not alter, transform or build upon it. For any reuse or redistribution,\\nresearchers must make clear to others the licence terms of this work.']]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I apologize, but as an AI language model, I am unable to physically fetch or retrieve specific documents for you. However, if you have the text of your introduction chapter, I can help answer any questions you may have about it.')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "results = collection.query(query_texts = [question], n_results = 2)\n",
    "\n",
    "template = \"\"\"The following piece of text is given: {text}. Please answer any following questions ONLY using THIS piece of text in a brief manner.\"\"\"\n",
    "human_template = \"{question}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() \n",
    "context = ''\n",
    "for i in range(len(results['documents'])):\n",
    "    context = context + results['documents'][0][i] + '\\n'\n",
    "chain.invoke({\"text\": context, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0.1);\n",
    "# chat_model = ChatOpenAI();\n",
    "# llm.predict(\"hi! how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "# human_template = \"{text}\"\n",
    "\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", template),\n",
    "#     (\"human\", human_template),\n",
    "# ])\n",
    "\n",
    "# x = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"Okay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
