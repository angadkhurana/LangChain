{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import chromadb\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\angad\\OneDrive\\Desktop\\AML\\thesis\\MSc_Thesis_02298815.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-17 19:03:25 - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name='angad_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=angad_embedding)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Abstract\\nElectricity is the backbone of today’s society. It powers everything from houses to industries and\\nbusinesses. The core infrastructure behind electricity is the grid which requires a continual balance\\nbetween supply and demand for optimal functioning. This necessitates the accurate short-term\\nforecasting of the load (STLF) consumed. In recent years, deep learning has been employed with\\nfederated learning for STLF as federated learning allows collaboration of clients without the need\\nto aggregate their data which is difficult due to privacy-related and logistical concerns.\\nThisstudyanalysestheimpactofthenetworktopologyoffederatedlearningontheperformance\\nof the predictive model for the case of STLF. Furthermore, since not all clients benefit from fed-\\nerated learning, the study of predictive models for distinguishing benefitting from non-benefitting\\nclients is also carried out.\\nThe ring topology results in the best performance in terms of the number of benefitting clients', metadata={'start_index': 0}),\n",
       " Document(page_content='The ring topology results in the best performance in terms of the number of benefitting clients\\nand reduction in loss metrics, however with a slower convergence speed. In terms of prediction of\\nthe non-benefitting clients, random forest with class weights was discovered to be the most effective\\nachieving a balanced accuracy of 86%, precision of 82%, and a recall of 82% as well.\\nThis research establishes the importance of STLF, identifies the best network topology for\\noptimal performance, and proves the feasibility of predicting the non-benefitting clients.', metadata={'start_index': 898})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([pages[1].page_content])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n",
      "['1', '2']\n",
      "['3']\n",
      "['4']\n",
      "['5']\n",
      "['6']\n",
      "['7']\n",
      "['8']\n",
      "['9']\n",
      "['10', '11']\n",
      "['12']\n",
      "['13']\n",
      "['14']\n",
      "['15', '16', '17']\n",
      "['18']\n",
      "['19']\n",
      "['20']\n",
      "['21', '22', '23']\n",
      "['24', '25', '26', '27']\n",
      "['28']\n",
      "['29']\n",
      "['30', '31']\n",
      "['32', '33', '34']\n",
      "['35', '36', '37']\n",
      "['38', '39', '40', '41']\n",
      "['42', '43', '44', '45']\n",
      "['46', '47', '48']\n",
      "['49', '50']\n",
      "['51', '52']\n",
      "['53', '54', '55']\n",
      "['56', '57', '58', '59']\n",
      "['60', '61', '62']\n",
      "['63', '64', '65']\n",
      "['66', '67', '68']\n",
      "['69']\n",
      "['70', '71']\n",
      "['72', '73', '74']\n",
      "['75', '76']\n",
      "['77', '78']\n",
      "['79', '80']\n",
      "['81', '82']\n",
      "['83', '84']\n",
      "['85', '86', '87']\n",
      "['88', '89', '90']\n",
      "['91', '92', '93']\n",
      "['94', '95', '96', '97']\n",
      "['98', '99', '100']\n",
      "['101', '102']\n",
      "['103', '104', '105']\n",
      "['106', '107', '108']\n"
     ]
    }
   ],
   "source": [
    "info = ''\n",
    "j = 0\n",
    "for i in pages[0:50]:\n",
    "    info = info + i.page_content\n",
    "    texts = text_splitter.create_documents([i.page_content])\n",
    "    documents = [i.page_content for i in texts]\n",
    "    metadatas = [i.metadata for i in texts]\n",
    "    ids = [str(j+i) for i in range(len(documents))]\n",
    "    j = j + len(documents)\n",
    "    print(ids)\n",
    "    collection.add(\n",
    "    documents = documents,\n",
    "    metadatas = metadatas,\n",
    "    ids = ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is my supervisor?\"\n",
    "results = collection.query(query_texts = [question], n_results = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['8', '20', '69', '18', '4']],\n",
       " 'distances': [[1.407895565032959,\n",
       "   1.641662836074829,\n",
       "   1.6523849964141846,\n",
       "   1.6579170227050781,\n",
       "   1.6685047149658203]],\n",
       " 'metadatas': [[{'start_index': 0},\n",
       "   {'start_index': 0},\n",
       "   {'start_index': 0},\n",
       "   {'start_index': 0},\n",
       "   {'start_index': 0}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Acknowledgments\\nI would firstly like to thank my supervisor Dr Fei Teng for encouraging thought provoking discus-\\nsions and providing incomparbale guidance throughout this thesis. I am also very thankful for Mr\\nXu’s for valuable inputs and insights on this thesis.\\nFinally, I am very grateful to my friends and family for continuously supporting me throughout\\nthis degree.',\n",
       "   'xvi',\n",
       "   '218 CHAPTER 2. LITERATURE REVIEW',\n",
       "   'xiv',\n",
       "   'Declaration of Originality\\nI hereby declare that the work presented in this thesis is my own unless otherwise stated. To the\\nbest of my knowledge the work is original and ideas developed in collaboration with others have\\nbeen appropriately referenced.']]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results['documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your supervisor is Dr Fei Teng.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "results = collection.query(query_texts = [question], n_results = 2)\n",
    "\n",
    "template = \"\"\"The following piece of text is given: {text}. Please answer any following questions ONLY using THIS piece of text in a brief manner.\"\"\"\n",
    "human_template = \"{question}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() \n",
    "context = ''\n",
    "for i in range(len(results['documents'])):\n",
    "    context = context + results['documents'][0][i] + '\\n'\n",
    "chain.invoke({\"text\": context, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0.1);\n",
    "# chat_model = ChatOpenAI();\n",
    "# llm.predict(\"hi! how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "# human_template = \"{text}\"\n",
    "\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", template),\n",
    "#     (\"human\", human_template),\n",
    "# ])\n",
    "\n",
    "# x = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"Okay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
